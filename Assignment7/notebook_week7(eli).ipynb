{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHoSDyYpdh-s"
   },
   "source": [
    "Assignment 7: Neural Networks using Keras and Tensorflow Please see the associated document for questions\n",
    "\n",
    "If you have problems with Keras and Tensorflow on your local installation please make sure they are updated. On Google Colab this notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "02ZYZ-WmdhwH"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJRCoRmew8Zd",
    "outputId": "8a74f963-06c8-4ba7-fb03-889e43dfa15e"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters data-loading and formatting\n",
    "\n",
    "\n",
    "batch_size = 128 #specifies the number of samples used in each training iteration.\n",
    "num_classes = 10 # The amount of neurons in the output layer, each corresponding to a specific class. [0,...,9]\n",
    "epochs = 10 # The number of times the entire training data set is used during training.\n",
    "\n",
    "img_rows, img_cols = 28, 28 # image dimensions.\n",
    "\n",
    "(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n",
    "\n",
    "#Makes sure that the input data is shaped in a way that works with the Keras framework.\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.1: See comments in the block below for explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I3g1RrZ0wpI"
   },
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "UswCCQLS0s1I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "#pixelvalues are normalized to the range 0 to 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "#one-hot encoding, so \n",
    "#the training and test labels (categorical data variables)\n",
    "#are converted to a binary matrix that can be fed to the machine learning algo\n",
    "# for example [0,1,2,3] = [[1,0,0,0],\n",
    "#                          [0,1,0,0]\n",
    "#                          [0,0,1,0]\n",
    "#                          [0,0,0,1]]\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(lbl_test, num_classes)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "#\n",
    "len(x_train) # = 60 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "N7Aer42gk1W9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.8576 - val_loss: 0.2586 - val_accuracy: 0.9266\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2381 - accuracy: 0.9304 - val_loss: 0.1985 - val_accuracy: 0.9397\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9457 - val_loss: 0.1605 - val_accuracy: 0.9507\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1535 - accuracy: 0.9556 - val_loss: 0.1504 - val_accuracy: 0.9542\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1310 - accuracy: 0.9619 - val_loss: 0.1263 - val_accuracy: 0.9606\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1119 - accuracy: 0.9679 - val_loss: 0.1077 - val_accuracy: 0.9663\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0994 - accuracy: 0.9712 - val_loss: 0.1044 - val_accuracy: 0.9672\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9741 - val_loss: 0.1015 - val_accuracy: 0.9689\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9768 - val_loss: 0.0940 - val_accuracy: 0.9719\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0727 - accuracy: 0.9786 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Test loss: 0.09230711311101913, Test accuracy 0.972000002861023\n"
     ]
    }
   ],
   "source": [
    "## Define model ##\n",
    "model = Sequential() #Allows us to build NN by stacking layers on top of each other\n",
    "\n",
    "#converts 2D input to a 1D vector\n",
    "#används som transition layer mellan convolutional layers and fully connected layers\n",
    "#snarare input layer??\n",
    "model.add(Flatten())\n",
    "\n",
    "#relu introduces the non-linearity property to the model\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "#softmax to get probability dist over the classes (e.g. 0 to 9)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),\n",
    "        metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           verbose=1,\n",
    "           validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Network model, training, and changing hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_23 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "64*65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. How many layers does the network in the notebook have? How many neurons does each layer have? \n",
    "What activation functions and why are these appropriate for this application? What is the total number of parameters for the network? \n",
    "Why do the input and output layers have the dimensions they have?\n",
    "\n",
    "Answer:\n",
    "1. Input layer: 28x28=784 input neurons. Flattening layer that converts matrix input to a vector\n",
    "2. Dense layer, consists of 64 neurons\n",
    "3. Dense layer, consists of 64 neurons\n",
    "4. Output layer: 10 sigmoid neurons (representing the numbers 0 to 9)\n",
    "\n",
    "There are two different activation functions here, relu and softmax.\n",
    "The relu (rectified linear unit) function introduces non-linearity to the output of the previous layer. It does this by setting the negative values to 0 (whilst not changing the positive ones). Relu is easier to compute than that the sigmoid function and it greatly speeds up training, another positive aspect of is that it dosen't saturate it's neurons as badly as the Sigmoid function - Sigmoid has to squish an infinite range into a finite range $[0,1]$, this makes the partial derivative with respect to a certain weight very small, leading to minimal changes in weight connected to the neuron during backpropagation. This in term leads worse results and slower convergence.\n",
    "\n",
    "$$Relu(x) = max(0,x)$$\n",
    "\n",
    "Softmax activation function is used to get probability distributions over the classes. It uses the output of the previous layer and transforms it into a probability distribution (and at the same time ensuring that the probabilities add up to 1). It is important here because it lets us compare the predicted probabilities from the different numbers (classes).\n",
    "\n",
    "$$Softmax(X) = \\frac{exp(x_j)}{\\sum{i=1}{n}exp(x_i)} for j = 1,...,n $$ \n",
    "\n",
    "The total number of parameters is 55 050 (which we can see in the output of the cell above).\n",
    "For the input layer every neuron represents a pixel in the input image, every image consists of 784 pixels, hense 784 input neurons.\n",
    "Every neuron of the output layer represents a single class. we have 10 classes (binary digits from 0 to 9) hense we get 10 neurons.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. What loss function is used to train the network? What is the functional form (a mathematical expression) of the loss function? and how should we interpret it? Why is it appropriate for the problem at hand?\n",
    "\n",
    "The loss function is the categorical_crossentropy. It is appropriate since we are dealing with multiple classes. It based around using one-hot category encoding value in form of 0s and 1 to the output label.\n",
    "\n",
    "$$L_{CE}(s_1) = {-}\\sum_{i=1}^{n} T_i Log(S_i)$$ \n",
    "\n",
    "$S$ is a vector with the actual output values. $T$ is a vector with corresponding expected value or true value. For our case only one value in the vector $T$ will be equal to 1 and the rest is 0. \n",
    "This means that categorical crossentropy only depends on $S_i$ if  $T_i = 1$. as $S_i$ approaches 1, the value of $L_{CE}$ will go to zero, and as the value of $S_i$ approaches 0 $L_{CE}$ goes to infinity\n",
    "\n",
    "Categorical crossentropy is good for our model since we want our model to choose between 10 different classes, and the categorical crossentropy function works for an arbitrary number of classes, as opposed to the binary crossentropy function which can only put things into two categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Train the network for 10 epochs and plot the training and validation accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4741 - accuracy: 0.8653 - val_loss: 0.2628 - val_accuracy: 0.9211\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2281 - accuracy: 0.9334 - val_loss: 0.1871 - val_accuracy: 0.9441\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.9496 - val_loss: 0.1751 - val_accuracy: 0.9465\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1446 - accuracy: 0.9578 - val_loss: 0.1288 - val_accuracy: 0.9610\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9636 - val_loss: 0.1177 - val_accuracy: 0.9634\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1093 - accuracy: 0.9671 - val_loss: 0.1109 - val_accuracy: 0.9655\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9711 - val_loss: 0.1085 - val_accuracy: 0.9666\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.9737 - val_loss: 0.1009 - val_accuracy: 0.9696\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9756 - val_loss: 0.0912 - val_accuracy: 0.9701\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9780 - val_loss: 0.1024 - val_accuracy: 0.9678\n",
      "Test loss: 0.10239894688129425, Test accuracy 0.9678000211715698\n"
     ]
    }
   ],
   "source": [
    "## Define model ##\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(64, activation = 'relu'))\n",
    "model_2.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model_2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),\n",
    "        metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model_2.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           verbose=1,\n",
    "           validation_data=(x_test, y_test))\n",
    "score = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracy= fit_info.history['accuracy']\n",
    "label_1='training accuracy'\n",
    "label_2='validation accuracy'\n",
    "val_accuracy= fit_info.history['val_accuracy']\n",
    "x_epoch=list(range(1,11))\n",
    "plt.plot(x_epoch, train_accuracy, 'm', label = label_1, linewidth=3, alpha=0.3)\n",
    "plt.plot(x_epoch, val_accuracy, 'c', label = label_2, linewidth=3, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('Training and validation accuracy for the 10 epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Update the model to implement a three-layer neural network where the hidden layers have 500 and 300 hidden units respectively. Train for 40 epochs. What is the best validation accuracy you can achieve? – Geoff Hinton (a co-pioneer of Deep learning) claimed this network could reach a validation accuracy of 0.9847 (http://yann.lecun.com/exdb/mnist/) using weight decay (L2 regularization of weights 2 (kernels): https://keras.io/api/layers/regularizers/). Implement weight decay on hidden units and train and select 5 regularization factors from 0.000001 to 0.001. Train 3 replicates networks for each regularization factor. Plot the final validation accuracy with standard deviation (computed from the replicates) as a function of the regularization factor. How close do you get to Hintons result? – If you do not get the same results, what factors may influence this? (hint: What information is not given by Hinton on the MNIST database that may influence Model training)\n",
    "\n",
    "Answer: It is possible that amount of epochs is what gives us a result lower than Geoff Hintons since we do not know how many epochs he had. We also don't know the regularization factors between 0.000001 and 0.001 were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.0527 - accuracy: 0.8057 - val_loss: 3.3614 - val_accuracy: 0.9089\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.9877 - accuracy: 0.8828 - val_loss: 1.2773 - val_accuracy: 0.8655\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.0479 - accuracy: 0.8945 - val_loss: 0.9188 - val_accuracy: 0.9005\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8260 - accuracy: 0.9043 - val_loss: 0.7090 - val_accuracy: 0.9272\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7066 - accuracy: 0.9153 - val_loss: 0.6358 - val_accuracy: 0.9290\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6484 - accuracy: 0.9191 - val_loss: 0.6657 - val_accuracy: 0.9135\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6401 - accuracy: 0.9162 - val_loss: 0.5692 - val_accuracy: 0.9324\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5752 - accuracy: 0.9276 - val_loss: 0.5574 - val_accuracy: 0.9305\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5521 - accuracy: 0.9303 - val_loss: 0.5801 - val_accuracy: 0.9242\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5484 - accuracy: 0.9305 - val_loss: 0.5168 - val_accuracy: 0.9343\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5363 - accuracy: 0.9311 - val_loss: 0.5405 - val_accuracy: 0.9227\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5546 - accuracy: 0.9258 - val_loss: 0.5736 - val_accuracy: 0.9290\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5159 - accuracy: 0.9351 - val_loss: 0.4933 - val_accuracy: 0.9381\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4935 - accuracy: 0.9362 - val_loss: 0.4790 - val_accuracy: 0.9382\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5662 - accuracy: 0.9226 - val_loss: 0.4840 - val_accuracy: 0.9428\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5194 - accuracy: 0.9316 - val_loss: 0.4792 - val_accuracy: 0.9393\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4783 - accuracy: 0.9390 - val_loss: 0.4445 - val_accuracy: 0.9480\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4790 - accuracy: 0.9386 - val_loss: 0.4429 - val_accuracy: 0.9482\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4851 - accuracy: 0.9356 - val_loss: 0.4606 - val_accuracy: 0.9464\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4835 - accuracy: 0.9375 - val_loss: 0.4425 - val_accuracy: 0.9452\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4681 - accuracy: 0.9395 - val_loss: 0.4274 - val_accuracy: 0.9501\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5694 - accuracy: 0.9233 - val_loss: 0.4742 - val_accuracy: 0.9388\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4750 - accuracy: 0.9379 - val_loss: 0.4782 - val_accuracy: 0.9314\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4691 - accuracy: 0.9381 - val_loss: 0.4819 - val_accuracy: 0.9324\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4819 - accuracy: 0.9358 - val_loss: 0.4160 - val_accuracy: 0.9499\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4567 - accuracy: 0.9413 - val_loss: 0.4271 - val_accuracy: 0.9506\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5445 - accuracy: 0.9229 - val_loss: 0.4301 - val_accuracy: 0.9505\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4372 - accuracy: 0.9455 - val_loss: 0.4082 - val_accuracy: 0.9522\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4997 - accuracy: 0.9300 - val_loss: 0.4333 - val_accuracy: 0.9464\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4987 - accuracy: 0.9310 - val_loss: 0.5037 - val_accuracy: 0.9318\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4529 - accuracy: 0.9427 - val_loss: 0.3977 - val_accuracy: 0.9542\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4755 - accuracy: 0.9362 - val_loss: 0.4386 - val_accuracy: 0.9471\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4470 - accuracy: 0.9413 - val_loss: 0.4048 - val_accuracy: 0.9554\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7058 - accuracy: 0.8926 - val_loss: 0.5811 - val_accuracy: 0.9235\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5475 - accuracy: 0.9270 - val_loss: 0.4801 - val_accuracy: 0.9379\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5622 - accuracy: 0.9211 - val_loss: 0.5073 - val_accuracy: 0.9370\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4594 - accuracy: 0.9416 - val_loss: 0.4574 - val_accuracy: 0.9350\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4568 - accuracy: 0.9406 - val_loss: 0.6446 - val_accuracy: 0.8767\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4431 - accuracy: 0.9411 - val_loss: 0.4253 - val_accuracy: 0.9469\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4851 - accuracy: 0.9341 - val_loss: 0.4469 - val_accuracy: 0.9391\n",
      "Test loss: 0.4468632936477661, Test accuracy 0.9391000270843506\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.0485 - accuracy: 0.8046 - val_loss: 3.4261 - val_accuracy: 0.8930\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.0109 - accuracy: 0.8813 - val_loss: 1.6213 - val_accuracy: 0.7653\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.0701 - accuracy: 0.8936 - val_loss: 0.8916 - val_accuracy: 0.9075\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8460 - accuracy: 0.9028 - val_loss: 0.7572 - val_accuracy: 0.9080\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7145 - accuracy: 0.9133 - val_loss: 0.7031 - val_accuracy: 0.9087\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6446 - accuracy: 0.9205 - val_loss: 0.6119 - val_accuracy: 0.9206\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6348 - accuracy: 0.9176 - val_loss: 0.6902 - val_accuracy: 0.8964\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5992 - accuracy: 0.9226 - val_loss: 0.9810 - val_accuracy: 0.8106\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5615 - accuracy: 0.9278 - val_loss: 0.5091 - val_accuracy: 0.9420\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5512 - accuracy: 0.9283 - val_loss: 0.5088 - val_accuracy: 0.9340\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5468 - accuracy: 0.9259 - val_loss: 0.5016 - val_accuracy: 0.9400\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6041 - accuracy: 0.9141 - val_loss: 0.5408 - val_accuracy: 0.9321\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5162 - accuracy: 0.9336 - val_loss: 0.5030 - val_accuracy: 0.9373\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4866 - accuracy: 0.9385 - val_loss: 0.4453 - val_accuracy: 0.9494\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4943 - accuracy: 0.9354 - val_loss: 0.4692 - val_accuracy: 0.9394\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6282 - accuracy: 0.9084 - val_loss: 0.5182 - val_accuracy: 0.9333\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4944 - accuracy: 0.9344 - val_loss: 0.4380 - val_accuracy: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4782 - accuracy: 0.9383 - val_loss: 0.4360 - val_accuracy: 0.9497\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5127 - accuracy: 0.9298 - val_loss: 0.4623 - val_accuracy: 0.9439\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5657 - accuracy: 0.9190 - val_loss: 0.5089 - val_accuracy: 0.9335\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4824 - accuracy: 0.9362 - val_loss: 0.4446 - val_accuracy: 0.9473\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5207 - accuracy: 0.9261 - val_loss: 0.6276 - val_accuracy: 0.8826\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5416 - accuracy: 0.9241 - val_loss: 0.4349 - val_accuracy: 0.9492\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4453 - accuracy: 0.9431 - val_loss: 0.4036 - val_accuracy: 0.9543\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4721 - accuracy: 0.9366 - val_loss: 0.4334 - val_accuracy: 0.9429\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5018 - accuracy: 0.9313 - val_loss: 0.4162 - val_accuracy: 0.9499\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5792 - accuracy: 0.9215 - val_loss: 0.5728 - val_accuracy: 0.9106\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4439 - accuracy: 0.9443 - val_loss: 0.4107 - val_accuracy: 0.9522\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4634 - accuracy: 0.9380 - val_loss: 0.4592 - val_accuracy: 0.9332\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4378 - accuracy: 0.9437 - val_loss: 0.3927 - val_accuracy: 0.9553\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4302 - accuracy: 0.9436 - val_loss: 0.4079 - val_accuracy: 0.9510\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4424 - accuracy: 0.9409 - val_loss: 0.4192 - val_accuracy: 0.9483\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5814 - accuracy: 0.9158 - val_loss: 0.4774 - val_accuracy: 0.9352\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4207 - accuracy: 0.9483 - val_loss: 0.4431 - val_accuracy: 0.9345\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4673 - accuracy: 0.9355 - val_loss: 0.4001 - val_accuracy: 0.9526\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7319 - accuracy: 0.8767 - val_loss: 0.5340 - val_accuracy: 0.9320\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4804 - accuracy: 0.9357 - val_loss: 0.4104 - val_accuracy: 0.9509\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5572 - accuracy: 0.9189 - val_loss: 0.5236 - val_accuracy: 0.9300\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4778 - accuracy: 0.9361 - val_loss: 0.4856 - val_accuracy: 0.9312\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4319 - accuracy: 0.9427 - val_loss: 0.4283 - val_accuracy: 0.9442\n",
      "Test loss: 0.42827391624450684, Test accuracy 0.9441999793052673\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.0340 - accuracy: 0.8059 - val_loss: 3.4268 - val_accuracy: 0.8857\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.9978 - accuracy: 0.8853 - val_loss: 1.1876 - val_accuracy: 0.9080\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.0496 - accuracy: 0.8956 - val_loss: 0.9569 - val_accuracy: 0.8843\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8130 - accuracy: 0.9094 - val_loss: 0.7028 - val_accuracy: 0.9276\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7164 - accuracy: 0.9107 - val_loss: 0.6776 - val_accuracy: 0.9233\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6466 - accuracy: 0.9172 - val_loss: 0.6083 - val_accuracy: 0.9252\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5960 - accuracy: 0.9239 - val_loss: 0.5575 - val_accuracy: 0.9331\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5886 - accuracy: 0.9225 - val_loss: 0.5578 - val_accuracy: 0.9273\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5536 - accuracy: 0.9280 - val_loss: 0.6103 - val_accuracy: 0.9096\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5540 - accuracy: 0.9268 - val_loss: 0.6277 - val_accuracy: 0.8925\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5313 - accuracy: 0.9304 - val_loss: 0.5126 - val_accuracy: 0.9321\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5076 - accuracy: 0.9339 - val_loss: 0.6446 - val_accuracy: 0.8909\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5527 - accuracy: 0.9232 - val_loss: 0.5657 - val_accuracy: 0.9300\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5893 - accuracy: 0.9184 - val_loss: 0.9924 - val_accuracy: 0.8489\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5976 - accuracy: 0.9195 - val_loss: 0.5033 - val_accuracy: 0.9385\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5007 - accuracy: 0.9357 - val_loss: 0.4870 - val_accuracy: 0.9395\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5092 - accuracy: 0.9334 - val_loss: 0.4712 - val_accuracy: 0.9382\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5010 - accuracy: 0.9336 - val_loss: 0.5741 - val_accuracy: 0.9113\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4851 - accuracy: 0.9363 - val_loss: 0.4405 - val_accuracy: 0.9473\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6128 - accuracy: 0.9100 - val_loss: 1.0034 - val_accuracy: 0.8064\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5501 - accuracy: 0.9277 - val_loss: 0.4576 - val_accuracy: 0.9461\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4831 - accuracy: 0.9370 - val_loss: 0.5095 - val_accuracy: 0.9271\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5330 - accuracy: 0.9249 - val_loss: 0.4701 - val_accuracy: 0.9431\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4627 - accuracy: 0.9398 - val_loss: 0.4425 - val_accuracy: 0.9434\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5629 - accuracy: 0.9210 - val_loss: 0.4799 - val_accuracy: 0.9382\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4606 - accuracy: 0.9409 - val_loss: 0.4891 - val_accuracy: 0.9267\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4745 - accuracy: 0.9362 - val_loss: 0.4114 - val_accuracy: 0.9539\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4657 - accuracy: 0.9367 - val_loss: 0.4626 - val_accuracy: 0.9381\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8420 - accuracy: 0.8568 - val_loss: 0.7785 - val_accuracy: 0.8777\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6293 - accuracy: 0.9081 - val_loss: 0.5910 - val_accuracy: 0.9091\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5524 - accuracy: 0.9193 - val_loss: 0.4901 - val_accuracy: 0.9305\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5156 - accuracy: 0.9270 - val_loss: 0.4996 - val_accuracy: 0.9288\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.8854 - val_loss: 0.5438 - val_accuracy: 0.9217\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5099 - accuracy: 0.9257 - val_loss: 0.4804 - val_accuracy: 0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5036 - accuracy: 0.9249 - val_loss: 0.5540 - val_accuracy: 0.9121\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4647 - accuracy: 0.9334 - val_loss: 0.4857 - val_accuracy: 0.9263\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4834 - accuracy: 0.9289 - val_loss: 0.4430 - val_accuracy: 0.9402\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4482 - accuracy: 0.9366 - val_loss: 0.4903 - val_accuracy: 0.9180\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4862 - accuracy: 0.9275 - val_loss: 0.4378 - val_accuracy: 0.9332\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4439 - accuracy: 0.9369 - val_loss: 0.4476 - val_accuracy: 0.9303\n",
      "Test loss: 0.4475939869880676, Test accuracy 0.9302999973297119\n",
      "mean= 0.9378666679064432\n",
      "std= 0.005741269074370668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Define model ##\n",
    "regularization_factors=[0.001,0.0001,0.0009,0.00001,0.000001]\n",
    "val_acc_0_001=[]\n",
    "val_acc_0_0001=[]\n",
    "val_acc_0_0009=[]\n",
    "val_acc_0_00001=[]\n",
    "val_acc_0_000001=[]\n",
    "\n",
    "#regularization factor 0.001\n",
    "for _ in range(3):\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(64, activation = 'relu', kernel_regularizer=regularizers.L1(0.001)))\n",
    "    model_3.add(Dense(64, activation = 'relu',kernel_regularizer=regularizers.L1(0.001)))\n",
    "    model_3.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.001)))\n",
    "    model_3.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.001)))\n",
    "\n",
    "    model_3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_3.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1), metrics=['accuracy'],)\n",
    "    epochs = 40\n",
    "    fit_info = model_3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model_3.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    val_acc_0_001.append(fit_info.history['val_accuracy'][-1])\n",
    "\n",
    "mean_0_001= np.mean(val_acc_0_001)\n",
    "std_0_001=np.std(val_acc_0_001)\n",
    "#print(val_acc_0_001)\n",
    "print('mean=', mean_0_001)\n",
    "print('std=', std_0_001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5852 - accuracy: 0.8302 - val_loss: 1.3009 - val_accuracy: 0.9011\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.1484 - accuracy: 0.9383 - val_loss: 1.0847 - val_accuracy: 0.9432\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.0084 - accuracy: 0.9531 - val_loss: 0.9604 - val_accuracy: 0.9546\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8971 - accuracy: 0.9623 - val_loss: 0.8535 - val_accuracy: 0.9610\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8026 - accuracy: 0.9674 - val_loss: 0.7672 - val_accuracy: 0.9639\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7192 - accuracy: 0.9708 - val_loss: 0.6881 - val_accuracy: 0.9679\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6448 - accuracy: 0.9730 - val_loss: 0.6393 - val_accuracy: 0.9648\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5791 - accuracy: 0.9757 - val_loss: 0.5612 - val_accuracy: 0.9706\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5192 - accuracy: 0.9769 - val_loss: 0.5069 - val_accuracy: 0.9723\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4670 - accuracy: 0.9778 - val_loss: 0.4523 - val_accuracy: 0.9742\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4197 - accuracy: 0.9784 - val_loss: 0.4201 - val_accuracy: 0.9690\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3804 - accuracy: 0.9793 - val_loss: 0.3785 - val_accuracy: 0.9729\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3473 - accuracy: 0.9793 - val_loss: 0.3884 - val_accuracy: 0.9625\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3142 - accuracy: 0.9815 - val_loss: 0.3317 - val_accuracy: 0.9713\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3158 - accuracy: 0.9766 - val_loss: 0.2984 - val_accuracy: 0.9748\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2716 - accuracy: 0.9810 - val_loss: 0.2919 - val_accuracy: 0.9709\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2529 - accuracy: 0.9820 - val_loss: 0.2752 - val_accuracy: 0.9722\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2395 - accuracy: 0.9825 - val_loss: 0.2578 - val_accuracy: 0.9732\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2292 - accuracy: 0.9827 - val_loss: 0.2581 - val_accuracy: 0.9729\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2208 - accuracy: 0.9830 - val_loss: 0.2642 - val_accuracy: 0.9671\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2145 - accuracy: 0.9829 - val_loss: 0.2309 - val_accuracy: 0.9756\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2070 - accuracy: 0.9835 - val_loss: 0.2431 - val_accuracy: 0.9712\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2093 - accuracy: 0.9822 - val_loss: 0.2247 - val_accuracy: 0.9743\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1970 - accuracy: 0.9843 - val_loss: 0.2264 - val_accuracy: 0.9735\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1918 - accuracy: 0.9846 - val_loss: 0.2181 - val_accuracy: 0.9749\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1888 - accuracy: 0.9843 - val_loss: 0.2236 - val_accuracy: 0.9745\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1856 - accuracy: 0.9843 - val_loss: 0.2377 - val_accuracy: 0.9666\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1796 - accuracy: 0.9854 - val_loss: 0.2266 - val_accuracy: 0.9692\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1819 - accuracy: 0.9839 - val_loss: 0.2065 - val_accuracy: 0.9737\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1744 - accuracy: 0.9854 - val_loss: 0.2057 - val_accuracy: 0.9735\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1704 - accuracy: 0.9859 - val_loss: 0.2367 - val_accuracy: 0.9635\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1681 - accuracy: 0.9857 - val_loss: 0.2164 - val_accuracy: 0.9716\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3139 - accuracy: 0.9619 - val_loss: 1.0448 - val_accuracy: 0.7381\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3645 - accuracy: 0.9423 - val_loss: 0.2890 - val_accuracy: 0.9610\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2509 - accuracy: 0.9707 - val_loss: 0.2458 - val_accuracy: 0.9719\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2215 - accuracy: 0.9768 - val_loss: 0.2525 - val_accuracy: 0.9687\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2060 - accuracy: 0.9790 - val_loss: 0.2165 - val_accuracy: 0.9754\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9817 - val_loss: 0.2325 - val_accuracy: 0.9691\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1849 - accuracy: 0.9824 - val_loss: 0.1986 - val_accuracy: 0.9760\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1764 - accuracy: 0.9833 - val_loss: 0.2000 - val_accuracy: 0.9744\n",
      "Test loss: 0.19998995959758759, Test accuracy 0.974399983882904\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5837 - accuracy: 0.8319 - val_loss: 1.2289 - val_accuracy: 0.9283\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.1448 - accuracy: 0.9393 - val_loss: 1.1038 - val_accuracy: 0.9351\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.0058 - accuracy: 0.9548 - val_loss: 0.9526 - val_accuracy: 0.9583\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8978 - accuracy: 0.9619 - val_loss: 0.8545 - val_accuracy: 0.9625\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8029 - accuracy: 0.9669 - val_loss: 0.8084 - val_accuracy: 0.9550\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7204 - accuracy: 0.9700 - val_loss: 0.6863 - val_accuracy: 0.9693\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6445 - accuracy: 0.9739 - val_loss: 0.6201 - val_accuracy: 0.9713\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5798 - accuracy: 0.9751 - val_loss: 0.5775 - val_accuracy: 0.9665\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5208 - accuracy: 0.9763 - val_loss: 0.5185 - val_accuracy: 0.9682\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4686 - accuracy: 0.9773 - val_loss: 0.4660 - val_accuracy: 0.9718\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4279 - accuracy: 0.9773 - val_loss: 0.4360 - val_accuracy: 0.9646\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3825 - accuracy: 0.9792 - val_loss: 0.3870 - val_accuracy: 0.9697\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3477 - accuracy: 0.9793 - val_loss: 0.3508 - val_accuracy: 0.9723\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3176 - accuracy: 0.9804 - val_loss: 0.3399 - val_accuracy: 0.9659\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2958 - accuracy: 0.9801 - val_loss: 0.3061 - val_accuracy: 0.9719\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2716 - accuracy: 0.9815 - val_loss: 0.3160 - val_accuracy: 0.9608\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2581 - accuracy: 0.9805 - val_loss: 0.2928 - val_accuracy: 0.9656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2506 - accuracy: 0.9797 - val_loss: 0.2717 - val_accuracy: 0.9719\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2322 - accuracy: 0.9820 - val_loss: 0.2579 - val_accuracy: 0.9722\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2228 - accuracy: 0.9831 - val_loss: 0.2478 - val_accuracy: 0.9717\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2179 - accuracy: 0.9822 - val_loss: 0.2276 - val_accuracy: 0.9778\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2082 - accuracy: 0.9835 - val_loss: 0.2227 - val_accuracy: 0.9778\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2143 - accuracy: 0.9817 - val_loss: 0.2485 - val_accuracy: 0.9694\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1980 - accuracy: 0.9845 - val_loss: 0.2470 - val_accuracy: 0.9658\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1936 - accuracy: 0.9843 - val_loss: 0.2233 - val_accuracy: 0.9745\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1901 - accuracy: 0.9843 - val_loss: 0.2135 - val_accuracy: 0.9763\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9841 - val_loss: 0.2071 - val_accuracy: 0.9777\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1803 - accuracy: 0.9856 - val_loss: 0.2052 - val_accuracy: 0.9773\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1911 - accuracy: 0.9817 - val_loss: 0.2097 - val_accuracy: 0.9752\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1759 - accuracy: 0.9857 - val_loss: 0.2088 - val_accuracy: 0.9735\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1727 - accuracy: 0.9854 - val_loss: 0.2069 - val_accuracy: 0.9756\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3476 - accuracy: 0.9518 - val_loss: 0.2658 - val_accuracy: 0.9658\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2320 - accuracy: 0.9716 - val_loss: 0.2745 - val_accuracy: 0.9608\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2029 - accuracy: 0.9784 - val_loss: 0.2296 - val_accuracy: 0.9694\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1882 - accuracy: 0.9821 - val_loss: 0.2060 - val_accuracy: 0.9762\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1817 - accuracy: 0.9823 - val_loss: 0.2083 - val_accuracy: 0.9753\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1730 - accuracy: 0.9841 - val_loss: 0.2777 - val_accuracy: 0.9543\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1727 - accuracy: 0.9837 - val_loss: 0.2011 - val_accuracy: 0.9734\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1630 - accuracy: 0.9855 - val_loss: 0.2123 - val_accuracy: 0.9699\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1577 - accuracy: 0.9867 - val_loss: 0.1898 - val_accuracy: 0.9775\n",
      "Test loss: 0.18983091413974762, Test accuracy 0.9775000214576721\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5959 - accuracy: 0.8260 - val_loss: 1.2178 - val_accuracy: 0.9299\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.1470 - accuracy: 0.9381 - val_loss: 1.0628 - val_accuracy: 0.9470\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.0069 - accuracy: 0.9540 - val_loss: 0.9844 - val_accuracy: 0.9458\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8972 - accuracy: 0.9629 - val_loss: 0.8586 - val_accuracy: 0.9607\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8050 - accuracy: 0.9669 - val_loss: 0.7733 - val_accuracy: 0.9617\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7221 - accuracy: 0.9702 - val_loss: 0.6920 - val_accuracy: 0.9656\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6476 - accuracy: 0.9727 - val_loss: 0.6196 - val_accuracy: 0.9701\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5808 - accuracy: 0.9743 - val_loss: 0.5910 - val_accuracy: 0.9591\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5210 - accuracy: 0.9760 - val_loss: 0.5009 - val_accuracy: 0.9725\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4668 - accuracy: 0.9775 - val_loss: 0.4599 - val_accuracy: 0.9705\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4215 - accuracy: 0.9785 - val_loss: 0.4334 - val_accuracy: 0.9665\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3826 - accuracy: 0.9783 - val_loss: 0.3756 - val_accuracy: 0.9744\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3480 - accuracy: 0.9795 - val_loss: 0.3453 - val_accuracy: 0.9728\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3149 - accuracy: 0.9808 - val_loss: 0.3270 - val_accuracy: 0.9714\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2912 - accuracy: 0.9803 - val_loss: 0.3115 - val_accuracy: 0.9684\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2710 - accuracy: 0.9809 - val_loss: 0.2943 - val_accuracy: 0.9689\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2542 - accuracy: 0.9815 - val_loss: 0.2987 - val_accuracy: 0.9620\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2404 - accuracy: 0.9823 - val_loss: 0.2548 - val_accuracy: 0.9750\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2293 - accuracy: 0.9829 - val_loss: 0.2590 - val_accuracy: 0.9707\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2219 - accuracy: 0.9832 - val_loss: 0.2411 - val_accuracy: 0.9736\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2153 - accuracy: 0.9822 - val_loss: 0.2312 - val_accuracy: 0.9763\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2085 - accuracy: 0.9831 - val_loss: 0.2297 - val_accuracy: 0.9753\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2060 - accuracy: 0.9828 - val_loss: 0.2217 - val_accuracy: 0.9752\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1977 - accuracy: 0.9845 - val_loss: 0.2650 - val_accuracy: 0.9612\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1966 - accuracy: 0.9834 - val_loss: 0.2250 - val_accuracy: 0.9729\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9850 - val_loss: 0.2153 - val_accuracy: 0.9739\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9839 - val_loss: 0.2283 - val_accuracy: 0.9710\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2209 - accuracy: 0.9766 - val_loss: 0.2254 - val_accuracy: 0.9732\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1865 - accuracy: 0.9837 - val_loss: 0.2335 - val_accuracy: 0.9680\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1750 - accuracy: 0.9858 - val_loss: 0.2385 - val_accuracy: 0.9622\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1792 - accuracy: 0.9838 - val_loss: 0.2001 - val_accuracy: 0.9768\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1708 - accuracy: 0.9860 - val_loss: 0.1940 - val_accuracy: 0.9781\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1646 - accuracy: 0.9875 - val_loss: 0.1940 - val_accuracy: 0.9773\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1629 - accuracy: 0.9868 - val_loss: 0.1954 - val_accuracy: 0.9745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1609 - accuracy: 0.9874 - val_loss: 0.1938 - val_accuracy: 0.9743\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2016 - accuracy: 0.9778 - val_loss: 0.2225 - val_accuracy: 0.9709\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1719 - accuracy: 0.9840 - val_loss: 0.1959 - val_accuracy: 0.9747\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1603 - accuracy: 0.9861 - val_loss: 0.2034 - val_accuracy: 0.9721\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1583 - accuracy: 0.9862 - val_loss: 0.1941 - val_accuracy: 0.9745\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1523 - accuracy: 0.9876 - val_loss: 0.1927 - val_accuracy: 0.9745\n",
      "Test loss: 0.19270072877407074, Test accuracy 0.9745000004768372\n",
      "mean= 0.9742999970912933\n",
      "std= 0.003995003574680587\n"
     ]
    }
   ],
   "source": [
    "#regularization factor 0.0001\n",
    "for _ in range(3):\n",
    "    model_4 = Sequential()\n",
    "    model_4.add(Flatten())\n",
    "    model_4.add(Dense(64, activation = 'relu', kernel_regularizer=regularizers.L1(0.0001)))\n",
    "    model_4.add(Dense(64, activation = 'relu',kernel_regularizer=regularizers.L1(0.0001)))\n",
    "    model_4.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.0001)))\n",
    "    model_4.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.0001)))\n",
    "\n",
    "    model_4.add(Dense(num_classes, activation='softmax'))\n",
    "    model_4.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),metrics=['accuracy'],)\n",
    "    epochs = 40\n",
    "    fit_info = model_4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model_4.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    val_acc_0_0001.append(fit_info.history['val_accuracy'][-1])\n",
    "\n",
    "mean_0_0001= np.mean(val_acc_0_0001)\n",
    "std_0_0001=np.std(val_acc_0_0001)\n",
    "#print(val_acc_0_0001)\n",
    "print('mean=', mean_0_0001)\n",
    "print('std=', std_0_0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786999821662903"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_info.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 4.7704 - accuracy: 0.8202 - val_loss: 3.3932 - val_accuracy: 0.9210\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 2.5550 - accuracy: 0.9239 - val_loss: 1.8326 - val_accuracy: 0.9331\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.3862 - accuracy: 0.9317 - val_loss: 1.0387 - val_accuracy: 0.9324\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8643 - accuracy: 0.9346 - val_loss: 0.7147 - val_accuracy: 0.9513\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6833 - accuracy: 0.9411 - val_loss: 0.6023 - val_accuracy: 0.9515\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5891 - accuracy: 0.9450 - val_loss: 0.5615 - val_accuracy: 0.9448\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5296 - accuracy: 0.9472 - val_loss: 0.5443 - val_accuracy: 0.9351\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4808 - accuracy: 0.9519 - val_loss: 0.4427 - val_accuracy: 0.9573\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4562 - accuracy: 0.9520 - val_loss: 0.4383 - val_accuracy: 0.9528\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4864 - accuracy: 0.9449 - val_loss: 0.4254 - val_accuracy: 0.9546\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4144 - accuracy: 0.9556 - val_loss: 0.3801 - val_accuracy: 0.9621\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3933 - accuracy: 0.9580 - val_loss: 0.4040 - val_accuracy: 0.9514\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4342 - accuracy: 0.9485 - val_loss: 0.3797 - val_accuracy: 0.9596\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3968 - accuracy: 0.9546 - val_loss: 0.3735 - val_accuracy: 0.9602\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5052 - accuracy: 0.9330 - val_loss: 0.4298 - val_accuracy: 0.9461\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3849 - accuracy: 0.9576 - val_loss: 0.3594 - val_accuracy: 0.9603\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3597 - accuracy: 0.9593 - val_loss: 0.3576 - val_accuracy: 0.9586\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4377 - accuracy: 0.9425 - val_loss: 0.3369 - val_accuracy: 0.9654\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3698 - accuracy: 0.9570 - val_loss: 0.3409 - val_accuracy: 0.9615\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3462 - accuracy: 0.9603 - val_loss: 0.3807 - val_accuracy: 0.9494\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3433 - accuracy: 0.9597 - val_loss: 0.3304 - val_accuracy: 0.9628\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3676 - accuracy: 0.9551 - val_loss: 0.3206 - val_accuracy: 0.9660\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3722 - accuracy: 0.9538 - val_loss: 0.3824 - val_accuracy: 0.9574\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3585 - accuracy: 0.9582 - val_loss: 0.3475 - val_accuracy: 0.9580\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3386 - accuracy: 0.9604 - val_loss: 0.3642 - val_accuracy: 0.9512\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3588 - accuracy: 0.9565 - val_loss: 0.3133 - val_accuracy: 0.9670\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3211 - accuracy: 0.9627 - val_loss: 0.3350 - val_accuracy: 0.9571\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3213 - accuracy: 0.9620 - val_loss: 0.3995 - val_accuracy: 0.9323\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3240 - accuracy: 0.9620 - val_loss: 0.3403 - val_accuracy: 0.9575\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3385 - accuracy: 0.9580 - val_loss: 0.3087 - val_accuracy: 0.9659\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3217 - accuracy: 0.9621 - val_loss: 0.3023 - val_accuracy: 0.9656\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3828 - accuracy: 0.9491 - val_loss: 0.3229 - val_accuracy: 0.9609\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3175 - accuracy: 0.9628 - val_loss: 0.3369 - val_accuracy: 0.9582\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3144 - accuracy: 0.9627 - val_loss: 0.3012 - val_accuracy: 0.9662\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7779 - accuracy: 0.8798 - val_loss: 0.5876 - val_accuracy: 0.9134\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4728 - accuracy: 0.9387 - val_loss: 0.4247 - val_accuracy: 0.9487\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3805 - accuracy: 0.9559 - val_loss: 0.4449 - val_accuracy: 0.9328\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3509 - accuracy: 0.9592 - val_loss: 0.3482 - val_accuracy: 0.9586\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3435 - accuracy: 0.9582 - val_loss: 0.3660 - val_accuracy: 0.9518\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3509 - accuracy: 0.9558 - val_loss: 0.3465 - val_accuracy: 0.9558\n",
      "Test loss: 0.3464765250682831, Test accuracy 0.9557999968528748\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 4.7799 - accuracy: 0.8178 - val_loss: 3.3626 - val_accuracy: 0.9269\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.5597 - accuracy: 0.9219 - val_loss: 1.8433 - val_accuracy: 0.9276\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3746 - accuracy: 0.9306 - val_loss: 0.9866 - val_accuracy: 0.9420\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8484 - accuracy: 0.9333 - val_loss: 0.7669 - val_accuracy: 0.9229\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.9363 - val_loss: 0.6019 - val_accuracy: 0.9457\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6065 - accuracy: 0.9387 - val_loss: 0.5285 - val_accuracy: 0.9538\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5318 - accuracy: 0.9453 - val_loss: 0.4964 - val_accuracy: 0.9494\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4846 - accuracy: 0.9501 - val_loss: 0.4868 - val_accuracy: 0.9395\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5117 - accuracy: 0.9419 - val_loss: 0.4384 - val_accuracy: 0.9535\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4331 - accuracy: 0.9530 - val_loss: 0.4582 - val_accuracy: 0.9420\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4228 - accuracy: 0.9531 - val_loss: 0.4847 - val_accuracy: 0.9302\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4020 - accuracy: 0.9555 - val_loss: 0.3829 - val_accuracy: 0.9570\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3832 - accuracy: 0.9568 - val_loss: 0.4014 - val_accuracy: 0.9454\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3776 - accuracy: 0.9566 - val_loss: 0.3485 - val_accuracy: 0.9641\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3632 - accuracy: 0.9590 - val_loss: 0.3806 - val_accuracy: 0.9519\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3580 - accuracy: 0.9586 - val_loss: 0.3873 - val_accuracy: 0.9470\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3847 - accuracy: 0.9525 - val_loss: 0.3518 - val_accuracy: 0.9579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3546 - accuracy: 0.9579 - val_loss: 0.4730 - val_accuracy: 0.9184\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3428 - accuracy: 0.9609 - val_loss: 0.3331 - val_accuracy: 0.9632\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4041 - accuracy: 0.9478 - val_loss: 0.4153 - val_accuracy: 0.9388\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3399 - accuracy: 0.9610 - val_loss: 0.4117 - val_accuracy: 0.9326\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4029 - accuracy: 0.9477 - val_loss: 0.4287 - val_accuracy: 0.9357\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3336 - accuracy: 0.9604 - val_loss: 0.3374 - val_accuracy: 0.9573\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3220 - accuracy: 0.9614 - val_loss: 0.3404 - val_accuracy: 0.9559\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3159 - accuracy: 0.9624 - val_loss: 0.3224 - val_accuracy: 0.9563\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3167 - accuracy: 0.9616 - val_loss: 0.3077 - val_accuracy: 0.9654\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3586 - accuracy: 0.9540 - val_loss: 0.3260 - val_accuracy: 0.9594\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3131 - accuracy: 0.9625 - val_loss: 0.3051 - val_accuracy: 0.9628\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3641 - accuracy: 0.9511 - val_loss: 0.3637 - val_accuracy: 0.9529\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3319 - accuracy: 0.9592 - val_loss: 0.3271 - val_accuracy: 0.9590\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3077 - accuracy: 0.9640 - val_loss: 0.3035 - val_accuracy: 0.9615\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3053 - accuracy: 0.9621 - val_loss: 0.3638 - val_accuracy: 0.9412\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3000 - accuracy: 0.9641 - val_loss: 0.2889 - val_accuracy: 0.9660\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3010 - accuracy: 0.9631 - val_loss: 0.3556 - val_accuracy: 0.9450\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3143 - accuracy: 0.9606 - val_loss: 0.2983 - val_accuracy: 0.9631\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3098 - accuracy: 0.9614 - val_loss: 0.3052 - val_accuracy: 0.9643\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3013 - accuracy: 0.9639 - val_loss: 0.2953 - val_accuracy: 0.9645\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4713 - accuracy: 0.9326 - val_loss: 0.4603 - val_accuracy: 0.9392\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3959 - accuracy: 0.9500 - val_loss: 0.3718 - val_accuracy: 0.9514\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3423 - accuracy: 0.9572 - val_loss: 0.3147 - val_accuracy: 0.9632\n",
      "Test loss: 0.3146815598011017, Test accuracy 0.9631999731063843\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 4.7913 - accuracy: 0.8141 - val_loss: 3.3883 - val_accuracy: 0.9233\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 2.5621 - accuracy: 0.9238 - val_loss: 1.8225 - val_accuracy: 0.9371\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.3922 - accuracy: 0.9318 - val_loss: 1.0335 - val_accuracy: 0.9323\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.8652 - accuracy: 0.9344 - val_loss: 0.7514 - val_accuracy: 0.9330\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6856 - accuracy: 0.9406 - val_loss: 0.5962 - val_accuracy: 0.9525\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6284 - accuracy: 0.9382 - val_loss: 0.5767 - val_accuracy: 0.9414\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5437 - accuracy: 0.9457 - val_loss: 0.5114 - val_accuracy: 0.9475\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4865 - accuracy: 0.9512 - val_loss: 0.4619 - val_accuracy: 0.9523\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4517 - accuracy: 0.9536 - val_loss: 0.4783 - val_accuracy: 0.9395\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4794 - accuracy: 0.9449 - val_loss: 0.4232 - val_accuracy: 0.9551\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4160 - accuracy: 0.9558 - val_loss: 0.3922 - val_accuracy: 0.9593\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4031 - accuracy: 0.9555 - val_loss: 0.3646 - val_accuracy: 0.9622\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3762 - accuracy: 0.9594 - val_loss: 0.4019 - val_accuracy: 0.9445\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4182 - accuracy: 0.9493 - val_loss: 0.3715 - val_accuracy: 0.9601\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.6079 - accuracy: 0.9170 - val_loss: 0.7241 - val_accuracy: 0.8696\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4673 - accuracy: 0.9457 - val_loss: 0.4272 - val_accuracy: 0.9483\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4008 - accuracy: 0.9545 - val_loss: 0.3767 - val_accuracy: 0.9567\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3843 - accuracy: 0.9541 - val_loss: 0.3583 - val_accuracy: 0.9603\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3673 - accuracy: 0.9562 - val_loss: 0.3565 - val_accuracy: 0.9605\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3546 - accuracy: 0.9585 - val_loss: 0.3672 - val_accuracy: 0.9510\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3374 - accuracy: 0.9605 - val_loss: 1.0759 - val_accuracy: 0.7796\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3641 - accuracy: 0.9542 - val_loss: 0.3385 - val_accuracy: 0.9607\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3440 - accuracy: 0.9579 - val_loss: 0.3193 - val_accuracy: 0.9650\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3268 - accuracy: 0.9610 - val_loss: 0.3421 - val_accuracy: 0.9540\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3247 - accuracy: 0.9614 - val_loss: 0.3117 - val_accuracy: 0.9618\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4870 - accuracy: 0.9283 - val_loss: 0.5179 - val_accuracy: 0.9251\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4142 - accuracy: 0.9459 - val_loss: 0.3723 - val_accuracy: 0.9513\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3321 - accuracy: 0.9604 - val_loss: 0.3055 - val_accuracy: 0.9678\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3143 - accuracy: 0.9622 - val_loss: 0.3200 - val_accuracy: 0.9578\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3494 - accuracy: 0.9546 - val_loss: 0.4010 - val_accuracy: 0.9428\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3147 - accuracy: 0.9619 - val_loss: 0.3248 - val_accuracy: 0.9564\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3788 - accuracy: 0.9482 - val_loss: 0.3168 - val_accuracy: 0.9616\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4221 - accuracy: 0.9417 - val_loss: 0.3365 - val_accuracy: 0.9583\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3054 - accuracy: 0.9632 - val_loss: 0.4147 - val_accuracy: 0.9281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3068 - accuracy: 0.9622 - val_loss: 0.3370 - val_accuracy: 0.9477\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2933 - accuracy: 0.9650 - val_loss: 0.2828 - val_accuracy: 0.9655\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2955 - accuracy: 0.9630 - val_loss: 0.2944 - val_accuracy: 0.9641\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5054 - accuracy: 0.9222 - val_loss: 0.3455 - val_accuracy: 0.9545\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3172 - accuracy: 0.9608 - val_loss: 0.3440 - val_accuracy: 0.9486\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3000 - accuracy: 0.9625 - val_loss: 0.2891 - val_accuracy: 0.9648\n",
      "Test loss: 0.2891083359718323, Test accuracy 0.9648000001907349\n",
      "mean= 0.9612666567166647\n",
      "std= 0.003920314546686462\n"
     ]
    }
   ],
   "source": [
    "#regularization factor 0.0009\n",
    "for _ in range(3):\n",
    "    model_5 = Sequential()\n",
    "    model_5.add(Flatten())\n",
    "    model_5.add(Dense(64, activation = 'relu', kernel_regularizer=regularizers.L1(0.0005)))\n",
    "    model_5.add(Dense(64, activation = 'relu',kernel_regularizer=regularizers.L1(0.0005)))\n",
    "    model_5.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.0005)))\n",
    "    model_5.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.0005)))\n",
    "\n",
    "    model_5.add(Dense(num_classes, activation='softmax'))\n",
    "    model_5.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),metrics=['accuracy'],)\n",
    "    epochs = 40\n",
    "    fit_info = model_5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model_5.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    val_acc_0_0009.append(fit_info.history['val_accuracy'][-1])\n",
    "\n",
    "mean_0_0009= np.mean(val_acc_0_0009)\n",
    "std_0_0009=np.std(val_acc_0_0009)\n",
    "#print(val_acc_0_0009)\n",
    "print('mean=', mean_0_0009)\n",
    "print('std=', std_0_0009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6600 - accuracy: 0.8297 - val_loss: 0.3709 - val_accuracy: 0.9186\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3076 - accuracy: 0.9395 - val_loss: 0.2914 - val_accuracy: 0.9449\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2547 - accuracy: 0.9552 - val_loss: 0.2564 - val_accuracy: 0.9532\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2250 - accuracy: 0.9639 - val_loss: 0.2275 - val_accuracy: 0.9614\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2057 - accuracy: 0.9699 - val_loss: 0.2117 - val_accuracy: 0.9667\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1916 - accuracy: 0.9743 - val_loss: 0.2000 - val_accuracy: 0.9711\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1810 - accuracy: 0.9774 - val_loss: 0.2109 - val_accuracy: 0.9675\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1718 - accuracy: 0.9796 - val_loss: 0.2185 - val_accuracy: 0.9642\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1636 - accuracy: 0.9818 - val_loss: 0.2231 - val_accuracy: 0.9619\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1571 - accuracy: 0.9832 - val_loss: 0.1876 - val_accuracy: 0.9736\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1506 - accuracy: 0.9849 - val_loss: 0.1904 - val_accuracy: 0.9736\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1448 - accuracy: 0.9867 - val_loss: 0.1939 - val_accuracy: 0.9733\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1401 - accuracy: 0.9881 - val_loss: 0.1857 - val_accuracy: 0.9751\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1351 - accuracy: 0.9896 - val_loss: 0.1815 - val_accuracy: 0.9748\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1314 - accuracy: 0.9900 - val_loss: 0.1820 - val_accuracy: 0.9753\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1285 - accuracy: 0.9912 - val_loss: 0.1855 - val_accuracy: 0.9761\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1248 - accuracy: 0.9922 - val_loss: 0.1982 - val_accuracy: 0.9695\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1208 - accuracy: 0.9933 - val_loss: 0.1920 - val_accuracy: 0.9752\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1177 - accuracy: 0.9942 - val_loss: 0.1854 - val_accuracy: 0.9760\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1285 - accuracy: 0.9917 - val_loss: 0.1899 - val_accuracy: 0.9758\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1142 - accuracy: 0.9947 - val_loss: 0.2056 - val_accuracy: 0.9703\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1109 - accuracy: 0.9955 - val_loss: 0.1793 - val_accuracy: 0.9773\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1084 - accuracy: 0.9961 - val_loss: 0.1882 - val_accuracy: 0.9754\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1075 - accuracy: 0.9961 - val_loss: 0.1843 - val_accuracy: 0.9777\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1268 - accuracy: 0.9924 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9968 - val_loss: 0.1840 - val_accuracy: 0.9771\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1003 - accuracy: 0.9982 - val_loss: 0.1878 - val_accuracy: 0.9760\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0986 - accuracy: 0.9983 - val_loss: 0.1835 - val_accuracy: 0.9772\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0961 - accuracy: 0.9991 - val_loss: 0.1834 - val_accuracy: 0.9779\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0943 - accuracy: 0.9992 - val_loss: 0.1896 - val_accuracy: 0.9762\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0927 - accuracy: 0.9996 - val_loss: 0.1845 - val_accuracy: 0.9784\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0922 - accuracy: 0.9994 - val_loss: 0.1983 - val_accuracy: 0.9754\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9991 - val_loss: 0.1838 - val_accuracy: 0.9778\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0893 - accuracy: 0.9998 - val_loss: 0.1836 - val_accuracy: 0.9782\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0881 - accuracy: 0.9999 - val_loss: 0.1833 - val_accuracy: 0.9778\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0869 - accuracy: 0.9999 - val_loss: 0.1841 - val_accuracy: 0.9779\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.9999 - val_loss: 0.1860 - val_accuracy: 0.9784\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0850 - accuracy: 0.9999 - val_loss: 0.1825 - val_accuracy: 0.9785\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0841 - accuracy: 0.9999 - val_loss: 0.1825 - val_accuracy: 0.9785\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.1816 - val_accuracy: 0.9783\n",
      "Test loss: 0.18157610297203064, Test accuracy 0.9782999753952026\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6739 - accuracy: 0.8193 - val_loss: 0.3279 - val_accuracy: 0.9333\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3127 - accuracy: 0.9374 - val_loss: 0.2944 - val_accuracy: 0.9393\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2559 - accuracy: 0.9545 - val_loss: 0.2620 - val_accuracy: 0.9495\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2259 - accuracy: 0.9640 - val_loss: 0.2274 - val_accuracy: 0.9622\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2099 - accuracy: 0.9678 - val_loss: 0.2150 - val_accuracy: 0.9663\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1930 - accuracy: 0.9731 - val_loss: 0.1958 - val_accuracy: 0.9705\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1822 - accuracy: 0.9761 - val_loss: 0.1951 - val_accuracy: 0.9709\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1736 - accuracy: 0.9789 - val_loss: 0.2144 - val_accuracy: 0.9675\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1644 - accuracy: 0.9817 - val_loss: 0.1922 - val_accuracy: 0.9715\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1578 - accuracy: 0.9826 - val_loss: 0.1841 - val_accuracy: 0.9746\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1531 - accuracy: 0.9839 - val_loss: 0.1800 - val_accuracy: 0.9754\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1486 - accuracy: 0.9856 - val_loss: 0.1968 - val_accuracy: 0.9719\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1423 - accuracy: 0.9870 - val_loss: 0.1888 - val_accuracy: 0.9744\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1390 - accuracy: 0.9883 - val_loss: 0.1776 - val_accuracy: 0.9760\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9893 - val_loss: 0.1889 - val_accuracy: 0.9733\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1296 - accuracy: 0.9905 - val_loss: 0.1838 - val_accuracy: 0.9749\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1272 - accuracy: 0.9909 - val_loss: 0.1932 - val_accuracy: 0.9734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1248 - accuracy: 0.9921 - val_loss: 0.1815 - val_accuracy: 0.9764\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1199 - accuracy: 0.9933 - val_loss: 0.2211 - val_accuracy: 0.9678\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1185 - accuracy: 0.9934 - val_loss: 0.1962 - val_accuracy: 0.9711\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1151 - accuracy: 0.9944 - val_loss: 0.1782 - val_accuracy: 0.9771\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9951 - val_loss: 0.1859 - val_accuracy: 0.9760\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1100 - accuracy: 0.9957 - val_loss: 0.1803 - val_accuracy: 0.9758\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1088 - accuracy: 0.9960 - val_loss: 0.1810 - val_accuracy: 0.9772\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1067 - accuracy: 0.9962 - val_loss: 0.1907 - val_accuracy: 0.9732\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1035 - accuracy: 0.9972 - val_loss: 0.2015 - val_accuracy: 0.9718\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1022 - accuracy: 0.9972 - val_loss: 0.1766 - val_accuracy: 0.9778\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1007 - accuracy: 0.9974 - val_loss: 0.1791 - val_accuracy: 0.9768\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1476 - accuracy: 0.9876 - val_loss: 0.1822 - val_accuracy: 0.9752\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1007 - accuracy: 0.9971 - val_loss: 0.1804 - val_accuracy: 0.9765\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0968 - accuracy: 0.9984 - val_loss: 0.1895 - val_accuracy: 0.9736\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0951 - accuracy: 0.9985 - val_loss: 0.1763 - val_accuracy: 0.9785\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0923 - accuracy: 0.9994 - val_loss: 0.1838 - val_accuracy: 0.9768\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1363 - accuracy: 0.9906 - val_loss: 0.1896 - val_accuracy: 0.9692\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1054 - accuracy: 0.9940 - val_loss: 0.1887 - val_accuracy: 0.9746\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0922 - accuracy: 0.9988 - val_loss: 0.1682 - val_accuracy: 0.9787\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0888 - accuracy: 0.9996 - val_loss: 0.1719 - val_accuracy: 0.9791\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0873 - accuracy: 0.9997 - val_loss: 0.1795 - val_accuracy: 0.9781\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0862 - accuracy: 0.9998 - val_loss: 0.1712 - val_accuracy: 0.9803\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0848 - accuracy: 0.9999 - val_loss: 0.1703 - val_accuracy: 0.9785\n",
      "Test loss: 0.1702798306941986, Test accuracy 0.9785000085830688\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6684 - accuracy: 0.8253 - val_loss: 0.3496 - val_accuracy: 0.9253\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3023 - accuracy: 0.9408 - val_loss: 0.2708 - val_accuracy: 0.9469\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2489 - accuracy: 0.9574 - val_loss: 0.2382 - val_accuracy: 0.9584\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2211 - accuracy: 0.9656 - val_loss: 0.2357 - val_accuracy: 0.9599\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2023 - accuracy: 0.9706 - val_loss: 0.2221 - val_accuracy: 0.9619\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1887 - accuracy: 0.9743 - val_loss: 0.2083 - val_accuracy: 0.9675\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1790 - accuracy: 0.9771 - val_loss: 0.2236 - val_accuracy: 0.9653\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1693 - accuracy: 0.9799 - val_loss: 0.2349 - val_accuracy: 0.9594\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1633 - accuracy: 0.9811 - val_loss: 0.1888 - val_accuracy: 0.9718\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1537 - accuracy: 0.9847 - val_loss: 0.1883 - val_accuracy: 0.9728\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1485 - accuracy: 0.9858 - val_loss: 0.1898 - val_accuracy: 0.9725\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1455 - accuracy: 0.9866 - val_loss: 0.1866 - val_accuracy: 0.9747\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1398 - accuracy: 0.9880 - val_loss: 0.1901 - val_accuracy: 0.9730\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1367 - accuracy: 0.9886 - val_loss: 0.1939 - val_accuracy: 0.9711\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1316 - accuracy: 0.9904 - val_loss: 0.1890 - val_accuracy: 0.9737\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1275 - accuracy: 0.9915 - val_loss: 0.1807 - val_accuracy: 0.9759\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1239 - accuracy: 0.9923 - val_loss: 0.1892 - val_accuracy: 0.9740\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1221 - accuracy: 0.9928 - val_loss: 0.2022 - val_accuracy: 0.9708\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1192 - accuracy: 0.9936 - val_loss: 0.1886 - val_accuracy: 0.9744\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1157 - accuracy: 0.9943 - val_loss: 0.1847 - val_accuracy: 0.9748\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1152 - accuracy: 0.9941 - val_loss: 0.1908 - val_accuracy: 0.9744\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1102 - accuracy: 0.9959 - val_loss: 0.2202 - val_accuracy: 0.9685\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1712 - accuracy: 0.9882 - val_loss: 1.1039 - val_accuracy: 0.6717\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1764 - accuracy: 0.9739 - val_loss: 0.1946 - val_accuracy: 0.9716\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1232 - accuracy: 0.9906 - val_loss: 0.1812 - val_accuracy: 0.9759\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1124 - accuracy: 0.9940 - val_loss: 0.1826 - val_accuracy: 0.9761\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1072 - accuracy: 0.9956 - val_loss: 0.1880 - val_accuracy: 0.9760\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1037 - accuracy: 0.9967 - val_loss: 0.1929 - val_accuracy: 0.9747\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1011 - accuracy: 0.9972 - val_loss: 0.1888 - val_accuracy: 0.9760\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1107 - accuracy: 0.9952 - val_loss: 0.1883 - val_accuracy: 0.9754\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0970 - accuracy: 0.9983 - val_loss: 0.1902 - val_accuracy: 0.9749\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0945 - accuracy: 0.9988 - val_loss: 0.1862 - val_accuracy: 0.9760\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0931 - accuracy: 0.9992 - val_loss: 0.1921 - val_accuracy: 0.9748\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0922 - accuracy: 0.9990 - val_loss: 0.2004 - val_accuracy: 0.9760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0910 - accuracy: 0.9991 - val_loss: 0.1895 - val_accuracy: 0.9749\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0885 - accuracy: 0.9998 - val_loss: 0.1873 - val_accuracy: 0.9776\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0874 - accuracy: 0.9998 - val_loss: 0.1878 - val_accuracy: 0.9764\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0861 - accuracy: 0.9999 - val_loss: 0.1878 - val_accuracy: 0.9769\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9999 - val_loss: 0.1859 - val_accuracy: 0.9774\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 1.0000 - val_loss: 0.1843 - val_accuracy: 0.9772\n",
      "Test loss: 0.1843108832836151, Test accuracy 0.9771999716758728\n",
      "mean= 0.9779999852180481\n",
      "std= 0.000571559019880347\n"
     ]
    }
   ],
   "source": [
    "#regularization factor 0.00001\n",
    "for _ in range(3):\n",
    "    model_6 = Sequential()\n",
    "    model_6.add(Flatten())\n",
    "    model_6.add(Dense(64, activation = 'relu', kernel_regularizer=regularizers.L1(0.00001)))\n",
    "    model_6.add(Dense(64, activation = 'relu',kernel_regularizer=regularizers.L1(0.00001)))\n",
    "    model_6.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.00001)))\n",
    "    model_6.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.00001)))\n",
    "\n",
    "    model_6.add(Dense(num_classes, activation='softmax'))\n",
    "    model_6.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),metrics=['accuracy'],)\n",
    "    epochs = 40\n",
    "    fit_info = model_6.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model_6.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    val_acc_0_00001.append(fit_info.history['val_accuracy'][-1])\n",
    "\n",
    "mean_0_00001= np.mean(val_acc_0_00001)\n",
    "std_0_00001=np.std(val_acc_0_00001)\n",
    "#print(val_acc_0_00001)\n",
    "print('mean=', mean_0_00001)\n",
    "print('std=', std_0_00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5584 - accuracy: 0.8298 - val_loss: 0.2396 - val_accuracy: 0.9287\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2023 - accuracy: 0.9416 - val_loss: 0.1767 - val_accuracy: 0.9491\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1536 - accuracy: 0.9573 - val_loss: 0.1375 - val_accuracy: 0.9610\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1252 - accuracy: 0.9651 - val_loss: 0.1314 - val_accuracy: 0.9617\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1062 - accuracy: 0.9706 - val_loss: 0.1200 - val_accuracy: 0.9662\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0920 - accuracy: 0.9748 - val_loss: 0.1168 - val_accuracy: 0.9677\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0816 - accuracy: 0.9787 - val_loss: 0.1091 - val_accuracy: 0.9683\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0732 - accuracy: 0.9809 - val_loss: 0.1003 - val_accuracy: 0.9723\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9828 - val_loss: 0.1143 - val_accuracy: 0.9667\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0588 - accuracy: 0.9854 - val_loss: 0.0964 - val_accuracy: 0.9734\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0533 - accuracy: 0.9873 - val_loss: 0.1070 - val_accuracy: 0.9723\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0511 - accuracy: 0.9877 - val_loss: 0.1015 - val_accuracy: 0.9730\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0463 - accuracy: 0.9891 - val_loss: 0.0960 - val_accuracy: 0.9758\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0419 - accuracy: 0.9906 - val_loss: 0.0954 - val_accuracy: 0.9764\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0415 - accuracy: 0.9909 - val_loss: 0.0960 - val_accuracy: 0.9757\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0358 - accuracy: 0.9925 - val_loss: 0.1036 - val_accuracy: 0.9766\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0324 - accuracy: 0.9937 - val_loss: 0.1074 - val_accuracy: 0.9738\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0318 - accuracy: 0.9940 - val_loss: 0.1115 - val_accuracy: 0.9745\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0281 - accuracy: 0.9949 - val_loss: 0.1044 - val_accuracy: 0.9778\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0262 - accuracy: 0.9954 - val_loss: 0.1237 - val_accuracy: 0.9721\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0248 - accuracy: 0.9963 - val_loss: 0.1110 - val_accuracy: 0.9738\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0244 - accuracy: 0.9959 - val_loss: 0.1090 - val_accuracy: 0.9766\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 0.1238 - val_accuracy: 0.9741\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0212 - accuracy: 0.9972 - val_loss: 0.1113 - val_accuracy: 0.9765\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0178 - accuracy: 0.9986 - val_loss: 0.1327 - val_accuracy: 0.9730\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0163 - accuracy: 0.9992 - val_loss: 0.1144 - val_accuracy: 0.9783\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0150 - accuracy: 0.9995 - val_loss: 0.1169 - val_accuracy: 0.9790\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0160 - accuracy: 0.9990 - val_loss: 0.1207 - val_accuracy: 0.9766\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2445 - accuracy: 0.9578 - val_loss: 0.1478 - val_accuracy: 0.9568\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0842 - accuracy: 0.9766 - val_loss: 0.1174 - val_accuracy: 0.9703\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0555 - accuracy: 0.9855 - val_loss: 0.1062 - val_accuracy: 0.9731\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0435 - accuracy: 0.9898 - val_loss: 0.1000 - val_accuracy: 0.9770\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0364 - accuracy: 0.9919 - val_loss: 0.1120 - val_accuracy: 0.9719\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0319 - accuracy: 0.9937 - val_loss: 0.1061 - val_accuracy: 0.9754\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0284 - accuracy: 0.9944 - val_loss: 0.1135 - val_accuracy: 0.9768\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0265 - accuracy: 0.9955 - val_loss: 0.1202 - val_accuracy: 0.9750\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0249 - accuracy: 0.9959 - val_loss: 0.1110 - val_accuracy: 0.9777\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0212 - accuracy: 0.9972 - val_loss: 0.1148 - val_accuracy: 0.9773\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.1192 - val_accuracy: 0.9767\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0205 - accuracy: 0.9974 - val_loss: 0.1234 - val_accuracy: 0.9764\n",
      "Test loss: 0.12342509627342224, Test accuracy 0.9764000177383423\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5572 - accuracy: 0.8336 - val_loss: 0.2645 - val_accuracy: 0.9211\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2091 - accuracy: 0.9410 - val_loss: 0.1703 - val_accuracy: 0.9523\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1594 - accuracy: 0.9554 - val_loss: 0.1482 - val_accuracy: 0.9591\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1308 - accuracy: 0.9639 - val_loss: 0.1297 - val_accuracy: 0.9645\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1119 - accuracy: 0.9694 - val_loss: 0.1385 - val_accuracy: 0.9593\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0972 - accuracy: 0.9742 - val_loss: 0.1131 - val_accuracy: 0.9665\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0883 - accuracy: 0.9762 - val_loss: 0.1106 - val_accuracy: 0.9693\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0797 - accuracy: 0.9783 - val_loss: 0.1079 - val_accuracy: 0.9685\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0716 - accuracy: 0.9810 - val_loss: 0.1048 - val_accuracy: 0.9685\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0639 - accuracy: 0.9835 - val_loss: 0.1136 - val_accuracy: 0.9691\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0590 - accuracy: 0.9851 - val_loss: 0.1044 - val_accuracy: 0.9726\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0540 - accuracy: 0.9864 - val_loss: 0.0971 - val_accuracy: 0.9741\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0494 - accuracy: 0.9878 - val_loss: 0.1096 - val_accuracy: 0.9709\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0460 - accuracy: 0.9890 - val_loss: 0.0988 - val_accuracy: 0.9752\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0425 - accuracy: 0.9903 - val_loss: 0.1019 - val_accuracy: 0.9752\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0391 - accuracy: 0.9912 - val_loss: 0.1155 - val_accuracy: 0.9728\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0358 - accuracy: 0.9923 - val_loss: 0.1239 - val_accuracy: 0.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0337 - accuracy: 0.9933 - val_loss: 0.1370 - val_accuracy: 0.9652\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0305 - accuracy: 0.9941 - val_loss: 0.1072 - val_accuracy: 0.9754\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0271 - accuracy: 0.9953 - val_loss: 0.1131 - val_accuracy: 0.9744\n",
      "Epoch 21/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0275 - accuracy: 0.9948 - val_loss: 0.1090 - val_accuracy: 0.9752\n",
      "Epoch 22/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0250 - accuracy: 0.9961 - val_loss: 0.1111 - val_accuracy: 0.9754\n",
      "Epoch 23/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0244 - accuracy: 0.9964 - val_loss: 0.1155 - val_accuracy: 0.9752\n",
      "Epoch 24/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0234 - accuracy: 0.9966 - val_loss: 0.1136 - val_accuracy: 0.9749\n",
      "Epoch 25/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0197 - accuracy: 0.9980 - val_loss: 0.1239 - val_accuracy: 0.9743\n",
      "Epoch 26/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0866 - accuracy: 0.9855 - val_loss: 0.1075 - val_accuracy: 0.9746\n",
      "Epoch 27/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0245 - accuracy: 0.9962 - val_loss: 0.1130 - val_accuracy: 0.9754\n",
      "Epoch 28/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9974 - val_loss: 0.1183 - val_accuracy: 0.9740\n",
      "Epoch 29/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0186 - accuracy: 0.9981 - val_loss: 0.1146 - val_accuracy: 0.9779\n",
      "Epoch 30/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0160 - accuracy: 0.9991 - val_loss: 0.1516 - val_accuracy: 0.9714\n",
      "Epoch 31/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0157 - accuracy: 0.9992 - val_loss: 0.1222 - val_accuracy: 0.9759\n",
      "Epoch 32/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0143 - accuracy: 0.9997 - val_loss: 0.1210 - val_accuracy: 0.9764\n",
      "Epoch 33/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0135 - accuracy: 0.9997 - val_loss: 0.1272 - val_accuracy: 0.9755\n",
      "Epoch 34/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0130 - accuracy: 0.9999 - val_loss: 0.1304 - val_accuracy: 0.9766\n",
      "Epoch 35/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0127 - accuracy: 0.9999 - val_loss: 0.1299 - val_accuracy: 0.9757\n",
      "Epoch 36/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0128 - accuracy: 0.9998 - val_loss: 0.1320 - val_accuracy: 0.9748\n",
      "Epoch 37/40\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0153 - accuracy: 0.9993 - val_loss: 0.1844 - val_accuracy: 0.9675\n",
      "Epoch 38/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0234 - accuracy: 0.9962 - val_loss: 0.1370 - val_accuracy: 0.9735\n",
      "Epoch 39/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0242 - accuracy: 0.9963 - val_loss: 0.1383 - val_accuracy: 0.9754\n",
      "Epoch 40/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0168 - accuracy: 0.9986 - val_loss: 0.1360 - val_accuracy: 0.9748\n",
      "Test loss: 0.13595044612884521, Test accuracy 0.9747999906539917\n",
      "Epoch 1/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.5879 - accuracy: 0.8140 - val_loss: 0.2456 - val_accuracy: 0.9294\n",
      "Epoch 2/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2130 - accuracy: 0.9395 - val_loss: 0.1885 - val_accuracy: 0.9438\n",
      "Epoch 3/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1570 - accuracy: 0.9556 - val_loss: 0.1838 - val_accuracy: 0.9449\n",
      "Epoch 4/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1276 - accuracy: 0.9646 - val_loss: 0.1357 - val_accuracy: 0.9616\n",
      "Epoch 5/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1096 - accuracy: 0.9695 - val_loss: 0.1164 - val_accuracy: 0.9686\n",
      "Epoch 6/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0944 - accuracy: 0.9745 - val_loss: 0.1136 - val_accuracy: 0.9677\n",
      "Epoch 7/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0846 - accuracy: 0.9775 - val_loss: 0.1312 - val_accuracy: 0.9636\n",
      "Epoch 8/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0773 - accuracy: 0.9790 - val_loss: 0.1004 - val_accuracy: 0.9717\n",
      "Epoch 9/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9819 - val_loss: 0.1021 - val_accuracy: 0.9708\n",
      "Epoch 10/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0638 - accuracy: 0.9835 - val_loss: 0.1002 - val_accuracy: 0.9703\n",
      "Epoch 11/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0575 - accuracy: 0.9858 - val_loss: 0.1052 - val_accuracy: 0.9712\n",
      "Epoch 12/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0535 - accuracy: 0.9869 - val_loss: 0.0998 - val_accuracy: 0.9719\n",
      "Epoch 13/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0503 - accuracy: 0.9880 - val_loss: 0.1050 - val_accuracy: 0.9708\n",
      "Epoch 14/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0465 - accuracy: 0.9893 - val_loss: 0.0998 - val_accuracy: 0.9731\n",
      "Epoch 15/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0429 - accuracy: 0.9899 - val_loss: 0.0970 - val_accuracy: 0.9748\n",
      "Epoch 16/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0394 - accuracy: 0.9912 - val_loss: 0.0983 - val_accuracy: 0.9750\n",
      "Epoch 17/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0374 - accuracy: 0.9919 - val_loss: 0.0983 - val_accuracy: 0.9742\n",
      "Epoch 18/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0344 - accuracy: 0.9927 - val_loss: 0.1076 - val_accuracy: 0.9755\n",
      "Epoch 19/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0319 - accuracy: 0.9940 - val_loss: 0.1235 - val_accuracy: 0.9696\n",
      "Epoch 20/40\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0303 - accuracy: 0.9945 - val_loss: 0.1140 - val_accuracy: 0.9739\n",
      "Epoch 21/40\n",
      "  1/469 [..............................] - ETA: 1s - loss: 0.0259 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "#regularization factor 0.000001\n",
    "for _ in range(3):\n",
    "    model_7 = Sequential()\n",
    "    model_7.add(Flatten())\n",
    "    model_7.add(Dense(64, activation = 'relu', kernel_regularizer=regularizers.L1(0.000001)))\n",
    "    model_7.add(Dense(64, activation = 'relu',kernel_regularizer=regularizers.L1(0.000001)))\n",
    "    model_7.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.000001)))\n",
    "    model_7.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.000001)))\n",
    "\n",
    "    model_7.add(Dense(num_classes, activation='softmax'))\n",
    "    model_7.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),metrics=['accuracy'],)\n",
    "    epochs = 40\n",
    "    fit_info = model_7.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "    score = model_7.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    val_acc_0_000001.append(fit_info.history['val_accuracy'][-1])\n",
    "\n",
    "mean_0_000001= np.mean(val_acc_0_000001)\n",
    "std_0_000001=np.std(val_acc_0_000001)\n",
    "#print(val_acc_0_000001)\n",
    "print('mean=', mean_0_000001)\n",
    "print('std=', std_0_000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012087129334751768\n"
     ]
    }
   ],
   "source": [
    "print(np.std(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_0_0001' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m regularization_factors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.0001\u001b[39m,\u001b[38;5;241m0.0009\u001b[39m,\u001b[38;5;241m0.00001\u001b[39m,\u001b[38;5;241m0.000001\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m stds \u001b[38;5;241m=\u001b[39m [std_0_001,\u001b[43mstd_0_0001\u001b[49m,std_0_0009,std_0_00001,std_0_000001]\n\u001b[0;32m      4\u001b[0m xval\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.0001\u001b[39m,\u001b[38;5;241m0.0009\u001b[39m,\u001b[38;5;241m0.00001\u001b[39m,\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#aka yval:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std_0_0001' is not defined"
     ]
    }
   ],
   "source": [
    "regularization_factors=[0.001,0.0001,0.0009,0.00001,0.000001]\n",
    "\n",
    "stds = [std_0_001,std_0_0001,std_0_0009,std_0_00001,std_0_000001]\n",
    "xval=(0.001,0.0001,0.0009,0.00001,0.000001)\n",
    "#aka yval:\n",
    "means = [mean_0_001,mean_0_0001,mean_0_0009,mean_0_00001,mean_0_000001]\n",
    "\n",
    "#plt.errorbar(xval,means,yerr=std_0_001, label='line1')\n",
    "#plt.errorbar(xval,means,yerr=stds)\n",
    "\n",
    "plt.errorbar(xval,means,yerr=stds,fmt='o',ecolor='salmon', elinewidth=2, capsize=5)\n",
    "plt.title('Validation accuracy for the three-layer neural networks')\n",
    "plt.xlabel(\"Std dev for \")\n",
    "#plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "print('Maximum accuracy is', np.max(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_0_0001' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m regularization_factors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.0001\u001b[39m,\u001b[38;5;241m0.0009\u001b[39m,\u001b[38;5;241m0.00001\u001b[39m,\u001b[38;5;241m0.000001\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m stds \u001b[38;5;241m=\u001b[39m [std_0_001,\u001b[43mstd_0_0001\u001b[49m,std_0_0009,std_0_00001,std_0_000001]\n\u001b[0;32m      4\u001b[0m xval\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;241m0.0001\u001b[39m,\u001b[38;5;241m0.0009\u001b[39m,\u001b[38;5;241m0.00001\u001b[39m,\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#aka yval:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std_0_0001' is not defined"
     ]
    }
   ],
   "source": [
    "regularization_factors=[0.001,0.0001,0.0009,0.00001,0.000001]\n",
    "\n",
    "stds = [std_0_001,std_0_0001,std_0_0009,std_0_00001,std_0_000001]\n",
    "xval=(0.001,0.0001,0.0009,0.00001,0.000001)\n",
    "#aka yval:\n",
    "means = [mean_0_001,mean_0_0001,mean_0_0009,mean_0_00001,mean_0_000001]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, CTEs,\n",
    "       yerr=error,\n",
    "       align='center',\n",
    "       alpha=0.5,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "ax.set_ylabel(\"fooobbaaar\")\n",
    "ax.set_xticks(xval)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('Coefficent of Thermal Expansion (CTE) of Three Metals')\n",
    "ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Design a model that makes use of at least one convolutional layer – how performant a model can you get? -- According to the MNIST database it should be possible reach to 99% accuracy on the validation data. If you choose to use any layers apart from the convolutional layers and layers that you used in previous questions, you must describe\n",
    "what they do. If you do not reach 99% accuracy, report your best performance, and explain your attempts and thought process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = Sequential()\n",
    "\n",
    "\n",
    "#chose kernel_size=(3,3) because när jag googla så stod det\n",
    "# att det är vanligt för fine-grained patterns\n",
    "#(28,28,1) betyder 28x28 pixels i grayscale\n",
    "model_8.add(Conv2D(64, kernel_size=(3,3), activation = 'relu', input_shape = (28,28,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling layer is applied after the Convolutional layer and is used to reduce the dimensions of the feature map,which helps in preserving the important information or features of the input image, this reduces the computation time. Max pooling takes a set of values $S$ from the convolutional layer as input and outputs $max(s_0...s_n)$.\n",
    "We need the flattened and dense layers, since this is were the actual classification takes place. The convolutional layer simply filters and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.4456 - accuracy: 0.8729 - val_loss: 0.1290 - val_accuracy: 0.9675\n",
      "Epoch 2/60\n",
      "469/469 [==============================] - 21s 45ms/step - loss: 0.1101 - accuracy: 0.9744 - val_loss: 0.0840 - val_accuracy: 0.9806\n",
      "Epoch 3/60\n",
      "469/469 [==============================] - 21s 45ms/step - loss: 0.0852 - accuracy: 0.9826 - val_loss: 0.0724 - val_accuracy: 0.9853\n",
      "Epoch 4/60\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0715 - accuracy: 0.9858 - val_loss: 0.0701 - val_accuracy: 0.9865\n",
      "Epoch 5/60\n",
      "469/469 [==============================] - 22s 47ms/step - loss: 0.0636 - accuracy: 0.9890 - val_loss: 0.0603 - val_accuracy: 0.9889\n",
      "Epoch 6/60\n",
      "469/469 [==============================] - 22s 46ms/step - loss: 0.0574 - accuracy: 0.9909 - val_loss: 0.0582 - val_accuracy: 0.9903\n",
      "Epoch 7/60\n",
      "469/469 [==============================] - 22s 46ms/step - loss: 0.0521 - accuracy: 0.9923 - val_loss: 0.0656 - val_accuracy: 0.9878\n",
      "Epoch 8/60\n",
      "312/469 [==================>...........] - ETA: 7s - loss: 0.0482 - accuracy: 0.9939"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m model_8\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcategorical_crossentropy, optimizer\u001b[38;5;241m=\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m),metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n\u001b[0;32m     26\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m---> 27\u001b[0m fit_info \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m score \u001b[38;5;241m=\u001b[39m model_8\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Test accuracy \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(score[\u001b[38;5;241m0\u001b[39m], score[\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_8 = Sequential()\n",
    "\n",
    "\n",
    "#chose kernel_size=(3,3) because när jag googla så stod det\n",
    "# att det är vanligt för fine-grained patterns\n",
    "#(28,28,1) betyder 28x28 pixels i grayscale\n",
    "model_8.add(Conv2D(64, kernel_size=(3,3), activation = 'relu', input_shape = (28,28,1)))\n",
    "\n",
    "\n",
    "model_8.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_8.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
    "model_8.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model_8.add(Flatten())\n",
    "#A Dense layer is just a fully connected layer were a every neuron is connected to every single neuron in the layer before.\n",
    "model_8.add(Dense(500, activation = 'relu',kernel_regularizer=regularizers.L1(0.000001)))\n",
    "model_8.add(Dense(300, activation = 'relu',kernel_regularizer=regularizers.L1(0.000001)))\n",
    "\n",
    "model_8.add(Dense(num_classes, activation='softmax'))\n",
    "model_8.compile(loss=keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),metrics=['accuracy'],)\n",
    "epochs = 60\n",
    "fit_info = model_8.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model_8.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Discuss the differences and potential benefits of using convolutional layers over fully\n",
    "connected ones for the application?\n",
    "\n",
    "One huge benefit is that the neurons can represent more specific features regardsless of were on the image they are located. for example the edge of a drawn number will be detected in the same way regardless of were in the full pixel space the edge is drawn. as opposed to a fully connected layer where it might not get recognized if it's to small or it exists in the the wrong part of the image.\n",
    "The number of weights associated with a convolutional network is alot smaller than a fully dense one, this is good when we are working with high dimensional data such as images.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_7_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
