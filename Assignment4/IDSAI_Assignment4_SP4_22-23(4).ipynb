{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "#Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import random as rd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [],
   "source": [
    "#Download and extract data\n",
    "#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "#!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "#!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "#!tar -xjf 20021010_spam.tar.bz2\n",
    "!tar -xjf \"20021010_easy_ham.tar.bz2\" -C /home/kokki/introDIT407/Module_4/easy_ham/\n",
    "!tar -xjf \"20021010_hard_ham.tar.bz2\" -C /home/kokki/introDIT407/Module_4/hard_ham/\n",
    "!tar -xjf \"20021010_spam.tar.bz2\" -C /home/kokki/introDIT407/Module_4/spam/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7,8M\r\n",
      "drwxrwxr-x 9 kokki kokki 4,0K apr 25 10:30 .\r\n",
      "drwxrwxr-x 7 kokki kokki 4,0K apr 24 14:42 ..\r\n",
      "-rw-rw-r-- 1 kokki kokki 1,6M jun 29  2004 20021010_easy_ham.tar.bz2\r\n",
      "-rw-rw-r-- 1 kokki kokki 1,6M jun 29  2004 20021010_easy_ham.tar.bz2.1\r\n",
      "-rw-rw-r-- 1 kokki kokki 998K dec 16  2004 20021010_hard_ham.tar.bz2\r\n",
      "-rw-rw-r-- 1 kokki kokki 998K dec 16  2004 20021010_hard_ham.tar.bz2.1\r\n",
      "-rw-rw-r-- 1 kokki kokki 1,2M jun 29  2004 20021010_spam.tar.bz2\r\n",
      "-rw-rw-r-- 1 kokki kokki 1,2M jun 29  2004 20021010_spam.tar.bz2.1\r\n",
      "drwx--x--x 5 kokki kokki 168K apr 24 14:45 easy_ham\r\n",
      "drwxrwxr-x 4 kokki kokki 4,0K apr 25 09:49 easyhardspam\r\n",
      "drwxrwxr-x 4 kokki kokki 4,0K apr 24 15:57 easyspam\r\n",
      "drwx--x--x 3 kokki kokki  20K apr 24 14:46 hard_ham\r\n",
      "drwxrwxr-x 4 kokki kokki 4,0K apr 25 09:44 hardspam\r\n",
      "-rw-rw-r-- 1 kokki kokki  48K apr 25 10:30 IDSAI_Assignment4_SP4_22-23.ipynb\r\n",
      "drwxrwxr-x 2 kokki kokki 4,0K apr 23 15:05 .ipynb_checkpoints\r\n",
      "-rw-rw-r-- 1 kokki kokki 6,5K apr 23 16:49 na‹ve_bayes_intro.ipynb\r\n",
      "-rw-rw-r-- 1 kokki kokki 9,0K apr 23 15:07 probability_intro.ipynb\r\n",
      "drwxr-xr-x 3 kokki kokki  36K apr 24 14:46 spam\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting data into folers\n",
    "easy_ham = load_files(\"/home/kokki/introDIT407/Module_4/easy_ham\", encoding=\"latin1\", shuffle=True)\n",
    "hard_ham= load_files(\"/home/kokki/introDIT407/Module_4/hard_ham\", encoding=\"latin1\", shuffle=True)\n",
    "spam = load_files(\"/home/kokki/introDIT407/Module_4/spam\", encoding=\"latin1\", shuffle=True)\n",
    "#easy_ham\n",
    "#spam har \"Recieved: from mail.\".... \n",
    "\n",
    "easyspam = datasets.load_files('./easyspam')\n",
    "hardspam = datasets.load_files('./hardspam')\n",
    "easyhardspam = datasets.load_files('./easyhardspam')\n",
    "\n",
    "\n",
    "\n",
    "#easy_ham_path = os.path.abspath(\"./easy_ham\")\n",
    "#print(easy_ham_path)\n",
    "#easy_ham = datasets.load_files(easy_ham_path)\n",
    "\n",
    "#easy_ham = load_files(\"/home/kokki/introDIT407/Module_4/easy_ham\", encoding=\"latin1\", shuffle=True)\n",
    "#print(easy_ham.data[:10])\n",
    "\n",
    "\n",
    "#print(easy_ham.keys())\n",
    "#print(easy_ham.DESCR)\n",
    "\n",
    "#easy_ham.data contains the email messages\n",
    "#easy_ham.target contains the labels (0 for ham and 1 for spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kokki/introDIT407/Module_4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # print current working directory#\n",
    "#print(os.listdir(\"./easy_ham\"))  # print contents of easy_ham directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "### 1. Preprocessing: \n",
    "\n",
    "##### 1.1 Look at a few emails from easy_ham, hard_ham and spam. Do you think you would be able to classify the emails just by inspection? How do you think a succesful model can learn the difference between the different classes of emails?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to tell the difference between easy_ham and spam, it's sometimes hard to tell the difference between hard-ham and spam, hard-ham often take the shape of poorly written newsletters, which resemble some of the strange offers embedded in spam messages\n",
    "\n",
    "& in spam email the mail adress that sents spam often starts with \"mail\",  \"Received: from mail....\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually looked at them and noticed that\n",
    "#spam has \"Recieved: from mail.\".... \n",
    "\n",
    "#!ls -lah /home/kokki/introDIT407/Module_4/easy_ham/\n",
    "#prints first 10 emails of easy_ham\n",
    "#print(easy_ham.data[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [],
   "source": [
    "# Write your code for here for looking a few emails\n",
    "\n",
    "#prints first 10 email of easy_ham\n",
    "#print(easy_ham.data[:10])\n",
    "\n",
    "#prints first 10 email of hard_ham\n",
    "#print(hard_ham.data[:10])\n",
    "\n",
    "#prints first 10 email of spam\n",
    "#print(spam.data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Note that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text (in the optional part further down can experiment with filtering out the headers and footers). We don’t want to train and test on the same data (it might help to reflect on why if you don't recall). Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`). Use only the easy_ham part as ham data for quesions 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for here for splitting the data\n",
    "\n",
    "# Split dataset into training set and test set \n",
    "x_easy_hamtrain, x_easy_hamtest, y_easy_hamtrain, y_easy_hamtest = train_test_split(easy_ham.data, easy_ham.target, test_size=0.2, random_state=42)\n",
    "#hamtrain, hamtest, y_hamtrain, y_hamtest = train_test_split(easy_ham.data, easy_ham.target, test_size=0.2,random_state=0)\n",
    "x_hard_hamtrain, x_hard_hamtest, y_hard_hamtrain_labels, y_hard_hamtest_labels = train_test_split(hard_ham.data, hard_ham.target, test_size=0.2, random_state=42)\n",
    "#spamtrain, spamtest, y_spamtrain, y_spamtest = train_test_split(spam.data,spam.target, test_size=0.2,radom_state=0)\n",
    "x_spamtrain, x_spamtest, y_spamtrain_labels, y_spamtest_labels = train_test_split(spam.data, spam.target, test_size=0.3, random_state=42)\n",
    "#spamtrain, spamtest, spamtrain_labels, spamtest_labels = train_test_split(other_emails.data, other_emails.target, test_size=0.2, random_state=42)\n",
    "\n",
    "#ham labeled 0 and spam labeled 1\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(easyspam.data, easyspam.target, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "#hamtrain, hamtest, spamtrain, spamtest = train_test_split(easy_ham.data, spam.target, test_size=0.3, random_state=42)\n",
    "\n",
    "#print(spamtest)\n",
    "#print(hamtest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for mnb: 0.9890829694323144\n",
      "Confusion matrix:\n",
      "[[753   1]\n",
      " [  9 153]]\n",
      "Accuracy for bnb: 0.9585152838427947\n",
      "Confusion matrix:\n",
      "[[748   6]\n",
      " [ 32 130]]\n"
     ]
    }
   ],
   "source": [
    "coun_vect = CountVectorizer(decode_error = 'ignore')\n",
    "hamtrain_vect = count_vect.transform(hamtrain)\n",
    "hamtest_vect = count_vect.transform(hamtest)\n",
    "\n",
    "mnb = MultinomialNB(force_alpha=True)\n",
    "mnb.fit(hamtrain_vect, spamtrain)\n",
    "\n",
    "y_pred_mnb = mnb.predict(hamtest_vect)\n",
    "\n",
    "print(\"Accuracy for mnb:\", accuracy_score(spamtest, y_pred_mnb))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(spamtest, y_pred_mnb))\n",
    "\n",
    "bnb = BernoulliNB(force_alpha=True)\n",
    "bnb.fit(hamtrain_vect, spamtrain)\n",
    "y_pred_bnb = bnb.predict(hamtest_vect)\n",
    "print(\"Accuracy for bnb:\", accuracy_score(spamtest, y_pred_bnb))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(spamtest, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for mnb: 0.6769911504424779\n"
     ]
    }
   ],
   "source": [
    "hardhamtrain, hardhamtest, spamtrain, spamtest = train_test_split(hardspam.data, hardspam.target, test_size=0.3, random_state=42)\n",
    "\n",
    "hardhamtrain_vect = count_vect.transform(hardhamtrain)\n",
    "hardhamtest_vect = count_vect.transform(hardhamtest)\n",
    "y_pred_mnb = mnb.predict(hardhamtest_vect)\n",
    "print(\"Accuracy for mnb:\", accuracy_score(spamtest, y_pred_mnb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "3047    0\n",
      "3048    0\n",
      "3049    0\n",
      "3050    0\n",
      "3051    0\n",
      "Name: money, Length: 3052, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "coun_vect = CountVectorizer(decode_error = 'ignore')\n",
    "count_matrix = coun_vect.fit_transform(easyspam.data)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = coun_vect.get_feature_names_out())\n",
    "print(df[\"money\"])\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(df, easyspam.target, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "### 2.1 Write a Python program that: \n",
    "1.\tUses the four datasets from Question 1 (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (use the [scikit-learn library](https://scikit-learn.org/stable/)) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. Use `CountVectorizer` ([Documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)) to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in scikit-learn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem:\n",
    "- Multinomial Naive Bayes  \n",
    "- Bernoulli Naive Bayes. \n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate before you start coding. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Answer the following questions:\n",
    "##### a) What does the CountVectorizer do?\n",
    "Answer 2.2.a\n",
    "##### b) What is the difference between Multinomial Naive Bayes and Bernoulli Naive Bayes\n",
    "Answer 2.2.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.1 Run the two models:\n",
    "Run (don't retrain) the two models from Question 2 on spam versus hard-ham. Does the performance differ compared to question 2 when the model was run on spam versus easy-ham? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gool_zb8Qzzy"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Retrain\n",
    "Retrain new Multinomial and Bernolli Naive Bayes classifers on the combined (easy+hard) ham and spam. Now evaluate on spam versus hard-ham as in 3.1. Also evaluate on spam versus easy-ham. Compare the performance with question 2 and 3.1. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Further improvements\n",
    "Do you have any suggestions for how performance could be further improved? You don't have to implement them, just present your ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.3:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
