{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3D82waLqItO"
   },
   "source": [
    "# DAT405/DIT407 Introduction to Data Science and AI \n",
    "## 2022-2023, Reading Period 4\n",
    "## Assignment 5: Reinforcement learning and classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 5 by Josefin Kokkinakis and Eli Uhlin, group 30.\n",
    "We have both worked around 15 hours each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiO_zpY7qItS"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUyGq4olqItS"
   },
   "source": [
    "The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n",
    "\n",
    "* The agent starts in state **S** (see table below)\n",
    "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n",
    "* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n",
    "* When reaching **F**, the game ends (absorbing state).\n",
    "* The numbers in the boxes represent the rewards you receive when moving into that box. \n",
    "* Assume no discount in this model: $\\gamma = 1$\n",
    "    \n",
    "    \n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|-1 |1|**F**|\n",
    "|0|-1|1|  \n",
    "|-1 |0|-1|  \n",
    "|**S**|-1|1|\n",
    "\n",
    "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1a:\n",
    "EENN which is unique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b)** What is the optimal policy (i.e. the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1b:\n",
    "in (0,0) N or E\n",
    "in (0,1) N or E\n",
    "in (0,2) N or E or S\n",
    "in (0,3) E\n",
    "in (1,0) E\n",
    "in (1,1) N or S or E or W\n",
    "in (1,2) N or E\n",
    "in (1,3) E\n",
    "in (2,0) N or W\n",
    "in (2,1) N or S\n",
    "in (2,2) N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c)** What is expected total reward for the policy in 1a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1c: 0 \n",
    "\n",
    "    since \n",
    "      (0,0) -> (1,0) means -1\n",
    "\n",
    "      (1,0) -> (2,0) means 1, and -1+1=0 total\n",
    "      \n",
    "      (2,0) -> (2,1) means -1, and -1+0=-1 total\n",
    "      \n",
    "      (2,1) -> (2,2) means +1, and -1+1=0 total\n",
    "      \n",
    "      (2,2) -> F     means that the total reward is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNkIk-k7qItT"
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJTFDikEqItT"
   },
   "source": [
    "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZdhW0AZDoZv"
   },
   "source": [
    "The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n",
    "\n",
    "```\n",
    "epsilon is a small value, threshold\n",
    "for x from i to infinity \n",
    "do\n",
    "    for each state s\n",
    "    do\n",
    "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
    "    end\n",
    "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n",
    "        for each state s,\n",
    "        do\n",
    "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
    "            return π, V_k \n",
    "        end\n",
    "end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz3UqgozqItU"
   },
   "source": [
    "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probability 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n",
    "\n",
    "**Reward**:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|10|0|  \n",
    "|0|0|0|  \n",
    "\n",
    "\n",
    "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|8|0|\n",
    "|8|2|8|  \n",
    "|0|8|0|  \n",
    "  \n",
    "**Iteration 2**:  \n",
    "  \n",
    "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
    "Action **S**: 0  \n",
    "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action **W**: 0\n",
    "\n",
    "Hence any action between **E** and **N** would be best at this stage.\n",
    "\n",
    "Similarly for cell (1,0):\n",
    "\n",
    "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
    "\n",
    "Similar calculations for remaining cells give us:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|5.76|10.88|5.76|\n",
    "|10.88|8.12|10.88|  \n",
    "|5.76|10.88|5.76|  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3vIdFpuqItU"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid. Make sure to consider that there may be several equally good actions for a state when presenting the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Define matrix\n",
    "2. Define grid?\n",
    "3. function that calculates what action to take in order to maximize the reward\n",
    "\n",
    "value of s= max_expected(possible_actions)\n",
    "\n",
    "expected_value(prob_of_moving*sum_of_reward_for_transitioning + discounted_value)\n",
    "\n",
    "\n",
    "moving_prob=0.8\n",
    "functionen V_k(x,y,V_prev, reward, moving_prob, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  1\n",
      "Gamma**generation:  0.9\n",
      "[0.0, 8.0, 0.0]\n",
      "[8.0, 2.0, 8.0]\n",
      "[0.0, 8.0, 0.0]\n",
      "Generation:  2\n",
      "Gamma**generation:  0.81\n",
      "[5.760000000000001, 10.88, 5.760000000000001]\n",
      "[10.88, 8.120000000000001, 10.88]\n",
      "[5.760000000000001, 10.88, 5.760000000000001]\n",
      "Generation:  3\n",
      "Gamma**generation:  0.7290000000000001\n",
      "[14.383360000000003, 10.224320000000002, 14.383360000000003]\n",
      "[10.224320000000002, 15.165680000000004, 10.224320000000002]\n",
      "[14.383360000000003, 10.224320000000002, 14.383360000000003]\n",
      "Generation:  4\n",
      "Gamma**generation:  0.6561\n",
      "[17.915917312000005, 19.007330432000007, 17.915917312000005]\n",
      "[19.007330432000007, 18.501979568000007, 19.007330432000007]\n",
      "[17.915917312000005, 19.007330432000007, 17.915917312000005]\n",
      "Generation:  5\n",
      "Gamma**generation:  0.5904900000000001\n",
      "[23.38362226682881, 26.382868934938887, 23.38362226682881]\n",
      "[26.382868934938887, 23.61698935606113, 26.382868934938887]\n",
      "[23.38362226682881, 26.382868934938887, 23.38362226682881]\n",
      "Generation:  6\n",
      "Gamma**generation:  0.531441\n",
      "[34.01366305238161, 32.87529063216685, 34.01366305238161]\n",
      "[32.87529063216685, 34.15843569008577, 32.87529063216685]\n",
      "[34.01366305238161, 32.87529063216685, 34.01366305238161]\n",
      "Generation:  7\n",
      "Gamma**generation:  0.4782969000000001\n",
      "[43.37529248564053, 42.18697531486646, 43.37529248564053]\n",
      "[42.18697531486646, 43.43735352655782, 42.18697531486646]\n",
      "[43.37529248564053, 42.18697531486646, 43.37529248564053]\n",
      "Generation:  8\n",
      "Gamma**generation:  0.4304672100000001\n",
      "[53.394538313486564, 54.55814780996276, 53.394538313486564]\n",
      "[54.55814780996276, 53.4294295617237, 54.55814780996276]\n",
      "[53.394538313486564, 54.55814780996276, 53.394538313486564]\n",
      "Generation:  9\n",
      "Gamma**generation:  0.3874204890000001\n",
      "[65.80995327284805, 66.2840705985854, 65.80995327284805]\n",
      "[66.2840705985854, 65.82536938868792, 66.2840705985854]\n",
      "[65.80995327284805, 66.2840705985854, 65.80995327284805]\n",
      "Generation:  10\n",
      "Gamma**generation:  0.3486784401000001\n",
      "[79.96849640164591, 79.1928120579513, 79.96849640164591]\n",
      "[79.1928120579513, 79.97666915512076, 79.1928120579513]\n",
      "[79.96849640164591, 79.1928120579513, 79.96849640164591]\n",
      "Generation:  11\n",
      "Gamma**generation:  0.31381059609000006\n",
      "[93.85616619033075, 93.74858706208825, 93.85616619033075]\n",
      "[93.74858706208825, 93.85981934608532, 93.74858706208825]\n",
      "[93.85616619033075, 93.74858706208825, 93.85616619033075]\n",
      "Generation:  12\n",
      "Gamma**generation:  0.2824295364810001\n",
      "[108.77400080931153, 109.2671224197125, 108.77400080931153]\n",
      "[109.2671224197125, 108.7758646398035, 109.2671224197125]\n",
      "[108.77400080931153, 109.2671224197125, 108.77400080931153]\n",
      "Generation:  13\n",
      "Gamma**generation:  0.2541865828329001\n",
      "[124.602511203778, 124.58683906124011, 124.602511203778]\n",
      "[124.58683906124011, 124.6033471150853, 124.58683906124011]\n",
      "[124.602511203778, 124.58683906124011, 124.602511203778]\n",
      "Generation:  14\n",
      "Gamma**generation:  0.2287679245496101\n",
      "[140.83759771423288, 140.54577598335132, 140.83759771423288]\n",
      "[140.54577598335132, 140.83801297581903, 140.54577598335132]\n",
      "[140.83759771423288, 140.54577598335132, 140.83759771423288]\n",
      "Generation:  15\n",
      "Gamma**generation:  0.20589113209464907\n",
      "[156.75569085601586, 156.80589454043675, 156.75569085601586]\n",
      "[156.80589454043675, 156.75587703798357, 156.80589454043675]\n",
      "[156.75569085601586, 156.80589454043675, 156.75569085601586]\n",
      "Generation:  16\n",
      "Gamma**generation:  0.18530201888518416\n",
      "[172.88701617688818, 173.05627019520648, 172.88701617688818]\n",
      "[173.05627019520648, 172.88710689584863, 173.05627019520648]\n",
      "[172.88701617688818, 173.05627019520648, 172.88701617688818]\n",
      "Generation:  17\n",
      "Gamma**generation:  0.16677181699666577\n",
      "[188.8572574292045, 188.80847974567206, 188.8572574292045]\n",
      "[188.80847974567206, 188.85729802767935, 188.80847974567206]\n",
      "[188.8572574292045, 188.80847974567206, 188.8572574292045]\n",
      "Generation:  18\n",
      "Gamma**generation:  0.15009463529699918\n",
      "[204.51197957168904, 204.4153859981585, 204.51197957168904]\n",
      "[204.4153859981585, 204.51199906961742, 204.4153859981585]\n",
      "[204.51197957168904, 204.4153859981585, 204.51197957168904]\n",
      "Generation:  19\n",
      "Gamma**generation:  0.13508517176729928\n",
      "[219.5027877283057, 219.54078806474723, 219.5027877283057]\n",
      "[219.54078806474723, 219.50279643330754, 219.54078806474723]\n",
      "[219.5027877283057, 219.54078806474723, 219.5027877283057]\n",
      "Generation:  20\n",
      "Gamma**generation:  0.12157665459056935\n",
      "[234.0903831221242, 234.1452758361333, 234.0903831221242]\n",
      "[234.1452758361333, 234.0903872568932, 234.1452758361333]\n",
      "[234.0903831221242, 234.1452758361333, 234.0903831221242]\n",
      "Generation:  21\n",
      "Gamma**generation:  0.10941898913151242\n",
      "[247.99845258729596, 247.97165554806725, 247.99845258729596]\n",
      "[247.97165554806725, 247.9984544288346, 247.97165554806725]\n",
      "[247.99845258729596, 247.97165554806725, 247.99845258729596]\n",
      "Generation:  22\n",
      "Gamma**generation:  0.09847709021836118\n",
      "[261.2676915976574, 261.2365187012331, 261.2676915976574]\n",
      "[261.2365187012331, 261.2676924649111, 261.2365187012331]\n",
      "[261.2676915976574, 261.2365187012331, 261.2676915976574]\n",
      "Generation:  23\n",
      "Gamma**generation:  0.08862938119652507\n",
      "[273.70344113961164, 273.72136279438365, 273.70344113961164]\n",
      "[273.72136279438365, 273.7034415250003, 273.72136279438365]\n",
      "[273.70344113961164, 273.72136279438365, 273.70344113961164]\n",
      "Generation:  24\n",
      "Gamma**generation:  0.07976644307687256\n",
      "[285.5021906082193, 285.5199420360987, 285.5021906082193]\n",
      "[285.5199420360987, 285.50219078850137, 285.5199420360987]\n",
      "[285.5021906082193, 285.5199420360987, 285.5021906082193]\n",
      "Generation:  25\n",
      "Gamma**generation:  0.0717897987691853\n",
      "[296.4924054735114, 296.4808032195066, 296.4924054735114]\n",
      "[296.4808032195066, 296.49240555346523, 296.4808032195066]\n",
      "[296.4924054735114, 296.4808032195066, 296.4924054735114]\n",
      "Generation:  26\n",
      "Gamma**generation:  0.06461081889226677\n",
      "[306.8008555372737, 306.79070458345177, 306.8008555372737]\n",
      "[306.79070458345177, 306.8008555744781, 306.79070458345177]\n",
      "[306.8008555372737, 306.79070458345177, 306.8008555372737]\n",
      "Generation:  27\n",
      "Gamma**generation:  0.058149737003040096\n",
      "[316.3052534942677, 316.31260842942004, 316.3052534942677]\n",
      "[316.31260842942004, 316.3052535107392, 316.31260842942004]\n",
      "[316.3052534942677, 316.31260842942004, 316.3052534942677]\n",
      "Generation:  28\n",
      "Gamma**generation:  0.05233476330273609\n",
      "[325.18614422762363, 325.19197821791965, 325.18614422762363]\n",
      "[325.19197821791965, 325.18614423525605, 325.19197821791965]\n",
      "[325.18614422762363, 325.19197821791965, 325.18614422762363]\n",
      "Generation:  29\n",
      "Gamma**generation:  0.047101286972462485\n",
      "[333.3299215862727, 333.3253254463773, 333.3299215862727]\n",
      "[333.3253254463773, 333.3299215896469, 333.3253254463773]\n",
      "[333.3299215862727, 333.3253254463773, 333.3299215862727]\n",
      "Generation:  30\n",
      "Gamma**generation:  0.04239115827521624\n",
      "[340.8909065257206, 340.8875360282385, 340.8909065257206]\n",
      "[340.8875360282385, 340.89090652727884, 340.8875360282385]\n",
      "[340.8909065257206, 340.8875360282385, 340.8909065257206]\n",
      "Generation:  31\n",
      "Gamma**generation:  0.038152042447694615\n",
      "[347.7768907440364, 347.77973415830115, 347.7768907440364]\n",
      "[347.77973415830115, 347.7768907447244, 347.77973415830115]\n",
      "[347.7768907440364, 347.77973415830115, 347.7768907440364]\n",
      "Generation:  32\n",
      "Gamma**generation:  0.03433683820292515\n",
      "[354.15669561137804, 354.1586528218979, 354.15669561137804]\n",
      "[354.1586528218979, 354.15669561169494, 354.1586528218979]\n",
      "[354.15669561137804, 354.1586528218979, 354.15669561137804]\n",
      "Generation:  33\n",
      "Gamma**generation:  0.030903154382632636\n",
      "[359.9398403946755, 359.93809402402314, 359.9398403946755]\n",
      "[359.93809402402314, 359.9398403948153, 359.93809402402314]\n",
      "[359.9398403946755, 359.93809402402314, 359.9398403946755]\n",
      "Generation:  34\n",
      "Gamma**generation:  0.027812838944369374\n",
      "[365.2814946612812, 365.28035271624344, 365.2814946612812]\n",
      "[365.28035271624344, 365.28149466134545, 365.28035271624344]\n",
      "[365.2814946612812, 365.28035271624344, 365.2814946612812]\n",
      "Generation:  35\n",
      "Gamma**generation:  0.025031555049932437\n",
      "[370.09793326993963, 370.09900014888433, 370.09793326993963]\n",
      "[370.09900014888433, 370.09793326996794, 370.09900014888433]\n",
      "[370.09793326993963, 370.09900014888433, 370.09793326993963]\n",
      "Generation:  36\n",
      "Gamma**generation:  0.022528399544939195\n",
      "[374.54472926027495, 374.54539840396615, 374.54472926027495]\n",
      "[374.54539840396615, 374.54472926028797, 374.54539840396615]\n",
      "[374.54472926027495, 374.54539840396615, 374.54472926027495]\n",
      "Generation:  37\n",
      "Gamma**generation:  0.020275559590445275\n",
      "[378.5366921411111, 378.5360429689253, 378.5366921411111]\n",
      "[378.5360429689253, 378.5366921411168, 378.5360429689253]\n",
      "[378.5366921411111, 378.5360429689253, 378.5366921411111]\n",
      "Generation:  38\n",
      "Gamma**generation:  0.01824800363140075\n",
      "[382.22029730404165, 382.219903715235, 382.22029730404165]\n",
      "[382.219903715235, 382.22029730404427, 382.219903715235]\n",
      "[382.22029730404165, 382.219903715235, 382.22029730404165]\n",
      "Generation:  39\n",
      "Gamma**generation:  0.016423203268260675\n",
      "[385.51092443079375, 385.51131824343577, 385.51092443079375]\n",
      "[385.51131824343577, 385.5109244307949, 385.51131824343577]\n",
      "[385.51092443079375, 385.51131824343577, 385.51092443079375]\n",
      "Generation:  40\n",
      "Gamma**generation:  0.014780882941434608\n",
      "[388.5513118811904, 388.55154415387744, 388.5513118811904]\n",
      "[388.55154415387744, 388.55131188119094, 388.55154415387744]\n",
      "[388.5513118811904, 388.55154415387744, 388.5513118811904]\n",
      "Generation:  41\n",
      "Gamma**generation:  0.013302794647291146\n",
      "[391.2543736851204, 391.2541353376189, 391.2543736851204]\n",
      "[391.2541353376189, 391.25437368512064, 391.2541353376189]\n",
      "[391.2543736851204, 391.2541353376189, 391.2543736851204]\n",
      "Generation:  42\n",
      "Gamma**generation:  0.011972515182562033\n",
      "[393.75627175077744, 393.75613428957837, 393.75627175077744]\n",
      "[393.75613428957837, 393.75627175077756, 393.75613428957837]\n",
      "[393.75627175077744, 393.75613428957837, 393.75627175077744]\n",
      "Generation:  43\n",
      "Gamma**generation:  0.01077526366430583\n",
      "[395.96843463227935, 395.96857862823424, 395.96843463227935]\n",
      "[395.96857862823424, 395.9684346322794, 395.96857862823424]\n",
      "[395.96843463227935, 395.96857862823424, 395.96843463227935]\n",
      "Generation:  44\n",
      "Gamma**generation:  0.009697737297875247\n",
      "[398.022827308999, 398.0229088547619, 398.022827308999]\n",
      "[398.0229088547619, 398.022827308999, 398.0229088547619]\n",
      "[398.022827308999, 398.0229088547619, 398.022827308999]\n",
      "Generation:  45\n",
      "Gamma**generation:  0.008727963568087723\n",
      "[399.82847127949105, 399.82838440743245, 399.82847127949105]\n",
      "[399.82838440743245, 399.82847127949105, 399.82838440743245]\n",
      "[399.82847127949105, 399.8283844074325, 399.82847127949105]\n",
      "Generation:  46\n",
      "Gamma**generation:  0.00785516721127895\n",
      "[401.51258026984806, 401.51253179732, 401.51258026984806]\n",
      "[401.51253179732, 401.51258026984806, 401.51253179732]\n",
      "[401.51258026984806, 401.51253179732, 401.51258026984806]\n",
      "Generation:  47\n",
      "Gamma**generation:  0.007069650490151055\n",
      "[402.98234993268807, 402.9824022843791, 402.98234993268807]\n",
      "[402.9824022843791, 402.98234993268807, 402.9824022843791]\n",
      "[402.98234993268807, 402.98240228437913, 402.98234993268807]\n",
      "Generation:  48\n",
      "Gamma**generation:  0.00636268544113595\n",
      "[404.36148615563604, 404.361515017088, 404.36148615563604]\n",
      "[404.361515017088, 404.36148615563604, 404.361515017088]\n",
      "[404.36148615563604, 404.361515017088, 404.36148615563604]\n",
      "Generation:  49\n",
      "Gamma**generation:  0.005726416897022355\n",
      "[405.55521690186856, 405.5551853806721, 405.55521690186856]\n",
      "[405.5551853806721, 405.55521690186856, 405.5551853806721]\n",
      "[405.55521690186856, 405.5551853806721, 405.55521690186856]\n",
      "Generation:  50\n",
      "Gamma**generation:  0.00515377520732012\n",
      "[406.6838873471372, 406.6838701385682, 406.6838873471372]\n",
      "[406.6838701385682, 406.6838873471372, 406.6838701385682]\n",
      "[406.6838873471372, 406.6838701385682, 406.6838873471372]\n",
      "Generation:  51\n",
      "Gamma**generation:  0.004638397686588108\n",
      "[407.65114894978643, 407.65116791571774, 407.65114894978643]\n",
      "[407.65116791571774, 407.65114894978643, 407.65116791571774]\n",
      "[407.65114894978643, 407.65116791571774, 407.65114894978643]\n",
      "Generation:  52\n",
      "Gamma**generation:  0.004174557917929297\n",
      "[408.57472179688295, 408.5747320692414, 408.57472179688295]\n",
      "[408.5747320692414, 408.57472179688295, 408.5747320692414]\n",
      "[408.57472179688295, 408.5747320692414, 408.57472179688295]\n",
      "Generation:  53\n",
      "Gamma**generation:  0.0037571021261363674\n",
      "[409.3567829967805, 409.3567715914922, 409.3567829967805]\n",
      "[409.3567715914922, 409.3567829967805, 409.3567715914922]\n",
      "[409.3567829967805, 409.3567715914922, 409.3567829967805]\n",
      "Generation:  54\n",
      "Gamma**generation:  0.0033813919135227306\n",
      "[410.1127252202346, 410.1127190825301, 410.1127252202346]\n",
      "[410.1127190825301, 410.1127252202346, 410.1127190825301]\n",
      "[410.1127252202346, 410.1127190825301, 410.1127252202346]\n",
      "Generation:  55\n",
      "Gamma**generation:  0.0030432527221704577\n",
      "[410.7435257086392, 410.74353256426457, 410.7435257086392]\n",
      "[410.74353256426457, 410.7435257086392, 410.74353256426457]\n",
      "[410.7435257086392, 410.74353256426457, 410.7435257086392]\n",
      "Generation:  56\n",
      "Gamma**generation:  0.002738927449953412\n",
      "[411.36271667948847, 411.36272034959313, 411.36271667948847]\n",
      "[411.36272034959313, 411.36271667948847, 411.36272034959313]\n",
      "[411.36271667948847, 411.36272034959313, 411.36271667948847]\n",
      "Generation:  57\n",
      "Gamma**generation:  0.002465034704958071\n",
      "[411.87022383778213, 411.8702197183756, 411.87022383778213]\n",
      "[411.8702197183756, 411.87022383778213, 411.8702197183756]\n",
      "[411.87022383778213, 411.8702197183756, 411.87022383778213]\n",
      "Generation:  58\n",
      "Gamma**generation:  0.002218531234462264\n",
      "[412.37799400314765, 412.3779918071776, 412.37799400314765]\n",
      "[412.3779918071776, 412.37799400314765, 412.3779918071776]\n",
      "[412.37799400314765, 412.3779918071776, 412.37799400314765]\n",
      "Generation:  59\n",
      "Gamma**generation:  0.0019966781110160375\n",
      "[412.78509399846035, 412.78509647302735, 412.78509399846035]\n",
      "[412.78509647302735, 412.78509399846035, 412.78509647302735]\n",
      "[412.78509399846035, 412.78509647302735, 412.78509399846035]\n",
      "Generation:  60\n",
      "Gamma**generation:  0.001797010299914434\n",
      "[413.2021912120648, 413.20219252668227, 413.2021912120648]\n",
      "[413.20219252668227, 413.2021912120648, 413.20219252668227]\n",
      "[413.2021912120648, 413.20219252668227, 413.2021912120648]\n",
      "Generation:  61\n",
      "Gamma**generation:  0.0016173092699229906\n",
      "[413.5276245735592, 413.5276230874016, 413.5276245735592]\n",
      "[413.5276230874016, 413.5276245735592, 413.5276230874016]\n",
      "[413.5276245735592, 413.5276230874016, 413.5276245735592]\n",
      "Generation:  62\n",
      "Gamma**generation:  0.0014555783429306916\n",
      "[413.87099432242803, 413.8709935350997, 413.87099432242803]\n",
      "[413.8709935350997, 413.87099432242803, 413.8709935350997]\n",
      "[413.87099432242803, 413.8709935350997, 413.87099432242803]\n",
      "Generation:  63\n",
      "Gamma**generation:  0.0013100205086376223\n",
      "[414.13004503981927, 414.1300459322014, 414.13004503981927]\n",
      "[414.1300459322014, 414.13004503981927, 414.1300459322014]\n",
      "[414.13004503981927, 414.1300459322014, 414.13004503981927]\n",
      "Generation:  64\n",
      "Gamma**generation:  0.0011790184577738603\n",
      "[414.4135125457458, 414.41351301744135, 414.4135125457458]\n",
      "[414.41351301744135, 414.4135125457458, 414.41351301744135]\n",
      "[414.4135125457458, 414.41351301744135, 414.4135125457458]\n",
      "Generation:  65\n",
      "Gamma**generation:  0.001061116611996474\n",
      "[414.61864693461223, 414.61864639884925, 414.61864693461223]\n",
      "[414.61864639884925, 414.61864693461223, 414.61864639884925]\n",
      "[414.61864693461223, 414.61864639884925, 414.61864693461223]\n",
      "Generation:  66\n",
      "Gamma**generation:  0.0009550049507968268\n",
      "[414.85347165655327, 414.853471373877, 414.85347165655327]\n",
      "[414.853471373877, 414.85347165655327, 414.853471373877]\n",
      "[414.85347165655327, 414.853471373877, 414.85347165655327]\n",
      "Generation:  67\n",
      "Gamma**generation:  0.0008595044557171441\n",
      "[415.0148336250732, 415.014833946693, 415.0148336250732]\n",
      "[415.014833946693, 415.0148336250732, 415.014833946693]\n",
      "[415.0148336250732, 415.014833946693, 415.0148336250732]\n",
      "Generation:  68\n",
      "Gamma**generation:  0.0007735540101454297\n",
      "[415.21017852932295, 415.21017869876283, 415.21017852932295]\n",
      "[415.21017869876283, 415.21017852932295, 415.21017869876283]\n",
      "[415.21017852932295, 415.21017869876283, 415.21017852932295]\n",
      "Generation:  69\n",
      "Gamma**generation:  0.0006961986091308868\n",
      "[415.3360213811285, 415.336021188078, 415.3360213811285]\n",
      "[415.336021188078, 415.3360213811285, 415.336021188078]\n",
      "[415.3360213811285, 415.336021188078, 415.3360213811285]\n",
      "Generation:  70\n",
      "Gamma**generation:  0.0006265787482177981\n",
      "[415.49933502517484, 415.49933492359156, 415.49933502517484]\n",
      "[415.49933492359156, 415.49933502517484, 415.49933492359156]\n",
      "[415.49933502517484, 415.49933492359156, 415.49933502517484]\n",
      "Generation:  71\n",
      "Gamma**generation:  0.0005639208733960184\n",
      "[415.59636427986266, 415.59636439573114, 415.59636427986266]\n",
      "[415.59636439573114, 415.59636427986266, 415.59636439573114]\n",
      "[415.59636427986266, 415.59636439573114, 415.59636427986266]\n",
      "final generation\n"
     ]
    }
   ],
   "source": [
    "# Answer 2a\n",
    "from random import shuffle\n",
    "from enum import Enum\n",
    "from math import sqrt\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "delta = 0.2\n",
    "from typing import List\n",
    "\n",
    "class State:\n",
    "    def __init__(self ,val, coord):\n",
    "        self.val = val\n",
    "\n",
    "        self.actions = []\n",
    "        self.coord = coord\n",
    "\n",
    "    def action_list_to_string(self):\n",
    "        newlst = []\n",
    "        for a in self.actions:\n",
    "            newlst.append(str(a.coord))\n",
    "\n",
    "        return ' '.join(newlst)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"(\" + str(self.get_x()) + \", \" + str(self.get_y()) + \") has value: \"+ str(self.val) + \" and actions: \" + str(self.action_list_to_string())\n",
    "\n",
    "    def get_coord(self):\n",
    "        return self.coord\n",
    "\n",
    "    def get_x(self):\n",
    "        return self.coord[0]\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.coord[1]\n",
    "\n",
    "    def get_val(self):\n",
    "        return self.val\n",
    "\n",
    "    def set_val(self, new_value):\n",
    "        self.val = new_value\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "\n",
    "    def add_action(self, state):\n",
    "        self.actions.append(state)\n",
    "\n",
    "    def get_max_action(self):\n",
    "        greatest_state: State = None\n",
    "        for a in self.actions:\n",
    "            if greatest_state is None:\n",
    "                greatest_state = a\n",
    "            elif a.get_val() > greatest_state.get_val():\n",
    "                greatest_state = a\n",
    "        return greatest_state\n",
    "\n",
    "    def find_action(self, coord):\n",
    "        for a in self.actions:\n",
    "            if a.get_coord() == coord:\n",
    "                return a\n",
    "\n",
    "\n",
    "def listcoord(rows, cols):\n",
    "    return [(x ,y) for x in range(rows) for y in range(cols)]\n",
    "\n",
    "\n",
    "def State_list(listval, listcoord):\n",
    "    state_list = []\n",
    "    for v, c in zip(listval, listcoord):\n",
    "        state_list.append(State(v ,c))\n",
    "    for s in state_list:\n",
    "        for t in state_list:\n",
    "            abs_x = abs(s.get_x() - t.get_x())\n",
    "            abs_y = abs(s.get_y() - t.get_y())\n",
    "            if ((abs_y == 1 and abs_x == 0) ^ (abs_y == 0 and abs_x == 1)):\n",
    "                s.add_action(t)\n",
    "    return state_list\n",
    "\n",
    "#0.8( 10 + 0.9 * 2) + 0.2(0 + 0.9 * 8)\n",
    "#Action N: 0.8( 10 + 0.9 * 2) + 0.2(0 + 0.9 * 8) = 10.88\n",
    "\n",
    "def value_function(s: State, o: State, generation):\n",
    "    s_val = s.get_val()\n",
    "    o_val = o.get_val()\n",
    "    action = s.get_max_action()\n",
    "    action_val = action.get_val()\n",
    "    old_action_val = o.find_action(action.get_coord()).get_val()\n",
    "    return (1-delta)*(old_action_val + (gamma**generation)*action_val) + delta*(o_val + (gamma**generation)*s_val)\n",
    "\n",
    "#0.8*(0+10)+0.2*(0+0.9*0)\n",
    "#0.8*(nuvarande_värde action + gamma* förra värde action) + 0.2(nuvarande + gamma*värdet_om_stå_kvar)\n",
    "\n",
    "\n",
    "#\n",
    "#TODO add new generation values to new list so that they don't effect other states during current gen.\n",
    "\n",
    "def new_Generation(state: List[State], old_state, generation):\n",
    "    next_gen_val = []\n",
    "    next_gen_coord = []\n",
    "    for s,o in zip(state, old_state):\n",
    "        next_gen_val.append(value_function(s, o, generation))\n",
    "        next_gen_coord.append(s.get_coord())\n",
    "    generation += 1\n",
    "    old_state = state\n",
    "    state = State_list(next_gen_val, next_gen_coord)\n",
    "    return (state, old_state, generation)\n",
    "\n",
    "def check_epsilon(state: List[State], old_state):\n",
    "    for s, o in zip(state, old_state):\n",
    "        if abs(s.get_val() - o.get_val()) >= epsilon:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def run_til_convergence(statelist: List[State], oldlist):\n",
    "    generation = 0\n",
    "    while not check_epsilon(statelist, oldlist):\n",
    "        (statelist, oldlist, generation) = new_Generation(statelist, oldlist, generation)\n",
    "        print(\"Generation: \", generation)\n",
    "        print(\"Gamma**generation: \", gamma ** generation)\n",
    "        print_matrix(statelist)\n",
    "    print(\"final generation\")\n",
    "\n",
    "\n",
    "def print_matrix(state: List[State]):\n",
    "    dimension = int(sqrt(len(state)))\n",
    "    mat = [[0 for i in range(dimension)] for i in range(dimension)]\n",
    "    for s in state:\n",
    "        mat[s.get_x()][s.get_y()] = s.get_val()\n",
    "    for row in mat:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "coord = listcoord(3,3)\n",
    "vals = [0 for x in range(9)]\n",
    "oldlist = State_list(vals, coord)\n",
    "vals[4] = 10\n",
    "statelist = State_list(vals, coord)\n",
    "generation = 0\n",
    "\n",
    "run_til_convergence(statelist, oldlist)\n",
    "\n",
    "                           \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2b:\n",
    "Because of gamma. During the first iteration when generation=0, we have gamma^0, which will be equal to 1. During the following iterations it will however decrease since gamma is less than 1. As gamma decreases with each iteration, so does the proportional value of each new iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c)** Describe your interpretation of the discount factor $\\gamma$. What would happen in the two extreme cases $\\gamma = 0$ and $\\gamma = 1$? Given some MDP, what would be important things to consider when deciding on which value of $\\gamma$ to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2c:\n",
    "The larger our discount factor gamma is, the smaller the future discounts are. If gamma is really small we get larger future discounts. So when gamma aproaches 0, future rewards will be very small, which creates a bias towards earlier iterations and when gamma equal 1, the future rewards are just as important as the intial reward.\n",
    "\n",
    "Chosing gamma values does therefore depends on how important furture rewards are to the learning agent. So it depends on if the agens want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKSybVI-UN1"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a)** What is the importance of exploration in reinforcement learning? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4a:\n",
    "Example: Imagine a person that is going to invest their money in a fund. If they find exploring to be very important and they don't aim for short term rewards, they aim for long term rewards. They will not just go for the first aalternative which at first glance may seem good but investigating more options gives the investor a greater understanding of how good the investment in the first fund is, in comparison to other funds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4b:\n",
    "Reinforcement learning is different from supervised learning since it does not require labeled data. It learns the data by exploring the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1iFSvirqItV"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a)** Give a summary of how a decision tree works and how it extends to random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5a:\n",
    "When using decision trees, we classify data by splitting it up depending on how it \"responds\" to a certain question. For example, lets say we want to predict how likely it is for us to \"go for a run\" on a monday morning. Factors that may inpact my decision about going for a run could be, answers to the questions: is it raining?, did I sleep well?, etc. The tree contains questions like these and splits the input space into smaller regions giving us a more complex and deeper tree that better fits our data.\n",
    "A disadvantage with using decision trees is that they are prone to overfitting since they are not \"flexible\" when it comes to classifying new samples. So when trees grow deeper, relationships between certain features may not be clear. This is why random forests was created. Random forests combines the predicitions of several decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b)** State at least one advantage and one drawback with using random forests over decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5b: An advantage with using random forests over decision trees is that they reduce overfitting. By combining the predictions of several decision trees they also have the flexability that tends to result in higher accuracy. \n",
    "A drawback with using random forests over decision trees is that random forests are more expensive since they contain several decision trees. Thus when making a prediction, all decision trees have to make predictions for the same given input.\n",
    "It is also more difficult to interpret than decision trees. Decision trees can be interpreted easily just by following the path in the tree that leads to the leaf."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ac54d0966a333a805ba7752ea123024b07a159dae64f707c7ddc090deb67544"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
